"""
Celeryå¼‚æ­¥ä»»åŠ¡
ç”¨äºNeural Systemçš„äº‹ä»¶å¤„ç†å’Œå‘é‡æ•°æ®åº“ç´¢å¼•
"""
from typing import Dict, Any
import asyncio
import os
import structlog
from celery import Task

from .celery_app import celery_app

logger = structlog.get_logger()


class CallbackTask(Task):
    """å¸¦å›è°ƒçš„ä»»åŠ¡åŸºç±»"""

    def on_success(self, retval, task_id, args, kwargs):
        """ä»»åŠ¡æˆåŠŸå›è°ƒ"""
        logger.info(
            "Celeryä»»åŠ¡æˆåŠŸ",
            task_id=task_id,
            task_name=self.name,
            result=retval,
        )

    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥å›è°ƒ"""
        logger.error(
            "Celeryä»»åŠ¡å¤±è´¥",
            task_id=task_id,
            task_name=self.name,
            error=str(exc),
            traceback=str(einfo),
        )

    def on_retry(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡é‡è¯•å›è°ƒ"""
        logger.warning(
            "Celeryä»»åŠ¡é‡è¯•",
            task_id=task_id,
            task_name=self.name,
            error=str(exc),
            retry_count=self.request.retries,
        )


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=int(os.getenv("CELERY_RETRY_BACKOFF_MAX", "600")),
    retry_jitter=True,
)
def process_neural_event(
    self,
    event_id: str,
    event_type: str,
    event_source: str,
    store_id: str,
    data: Dict[str, Any],
    priority: int = 0,
) -> Dict[str, Any]:
    """
    å¤„ç†ç¥ç»ç³»ç»Ÿäº‹ä»¶ï¼ˆå¼‚æ­¥ä»»åŠ¡ï¼‰

    Args:
        event_id: äº‹ä»¶ID
        event_type: äº‹ä»¶ç±»å‹
        event_source: äº‹ä»¶æ¥æº
        store_id: é—¨åº—ID
        data: äº‹ä»¶æ•°æ®
        priority: ä¼˜å…ˆçº§

    Returns:
        å¤„ç†ç»“æœ
    """
    async def _run():
        from datetime import datetime
        from ..services.vector_db_service import vector_db_service
        from ..core.database import AsyncSessionLocal
        from ..models.neural_event_log import NeuralEventLog, EventProcessingStatus

        # 1. å†™å…¥ DB â€” æ ‡è®°ä¸º processing
        async with AsyncSessionLocal() as session:
            log = NeuralEventLog(
                event_id=event_id,
                celery_task_id=self.request.id,
                event_type=event_type,
                event_source=event_source,
                store_id=store_id,
                priority=priority,
                data=data,
                processing_status=EventProcessingStatus.PROCESSING,
                queued_at=datetime.utcnow(),
                started_at=datetime.utcnow(),
            )
            session.add(log)
            await session.commit()

        logger.info(
            "å¼€å§‹å¤„ç†ç¥ç»ç³»ç»Ÿäº‹ä»¶",
            event_id=event_id,
            event_type=event_type,
            store_id=store_id,
        )

        actions_taken = []
        downstream_tasks = []
        vector_indexed = False
        wechat_sent = False

        try:
            # 2. å‘é‡åŒ–å­˜å‚¨ï¼ˆå…¨å±€ç´¢å¼• + é¢†åŸŸåˆ†å‰²ç´¢å¼•ï¼‰
            event_payload = {
                "event_id": event_id,
                "event_type": event_type,
                "event_source": event_source,
                "timestamp": datetime.utcnow(),
                "store_id": store_id,
                "data": data,
                "priority": priority,
            }
            await vector_db_service.index_event(event_payload)
            from ..services.domain_vector_service import domain_vector_service
            await domain_vector_service.index_neural_event(store_id, event_payload)
            vector_indexed = True
            actions_taken.append("vector_indexed")

            # 3. è§¦å‘ä¼å¾®æ¨é€
            from ..services.wechat_trigger_service import wechat_trigger_service
            try:
                await wechat_trigger_service.trigger_push(
                    event_type=event_type,
                    event_data=data,
                    store_id=store_id,
                )
                wechat_sent = True
                actions_taken.append("wechat_sent")
            except Exception as e:
                logger.warning("ä¼å¾®æ¨é€è§¦å‘å¤±è´¥", event_type=event_type, error=str(e))

            # 4. æ ¹æ®äº‹ä»¶ç±»å‹è§¦å‘ä¸‹æ¸¸ä»»åŠ¡
            if event_type.startswith("order."):
                t = index_order_to_vector_db.delay(data)
                downstream_tasks.append({"task_name": "index_order_to_vector_db", "task_id": t.id})
                actions_taken.append("dispatched:index_order_to_vector_db")
            elif event_type.startswith("dish."):
                t = index_dish_to_vector_db.delay(data)
                downstream_tasks.append({"task_name": "index_dish_to_vector_db", "task_id": t.id})
                actions_taken.append("dispatched:index_dish_to_vector_db")

            processed_at = datetime.utcnow()

            # 5. å†™å› DB â€” æ ‡è®°ä¸º completed
            async with AsyncSessionLocal() as session:
                db_log = await session.get(NeuralEventLog, event_id)
                if db_log:
                    db_log.processing_status = EventProcessingStatus.COMPLETED
                    db_log.vector_indexed = vector_indexed
                    db_log.wechat_sent = wechat_sent
                    db_log.downstream_tasks = downstream_tasks
                    db_log.actions_taken = actions_taken
                    db_log.processed_at = processed_at
                    await session.commit()

            logger.info("ç¥ç»ç³»ç»Ÿäº‹ä»¶å¤„ç†å®Œæˆ", event_id=event_id, event_type=event_type)
            return {
                "success": True,
                "event_id": event_id,
                "processed_at": processed_at.isoformat(),
                "actions_taken": actions_taken,
            }

        except Exception as e:
            logger.error("ç¥ç»ç³»ç»Ÿäº‹ä»¶å¤„ç†å¤±è´¥", event_id=event_id, error=str(e), exc_info=e)
            # å†™å› DB â€” æ ‡è®°ä¸º failed / retrying
            try:
                is_last_retry = self.request.retries >= self.max_retries
                async with AsyncSessionLocal() as session:
                    db_log = await session.get(NeuralEventLog, event_id)
                    if db_log:
                        db_log.processing_status = (
                            EventProcessingStatus.FAILED if is_last_retry
                            else EventProcessingStatus.RETRYING
                        )
                        db_log.error_message = str(e)
                        db_log.retry_count = self.request.retries + 1
                        await session.commit()
            except Exception as db_err:
                logger.warning("celery_tasks.status_update_failed", error=str(db_err))
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_SHORT", "30")),
)
def index_to_vector_db(
    self,
    collection_name: str,
    data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•æ•°æ®åˆ°å‘é‡æ•°æ®åº“ï¼ˆé€šç”¨ä»»åŠ¡ï¼‰

    Args:
        collection_name: é›†åˆåç§°
        data: è¦ç´¢å¼•çš„æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        try:
            from ..services.vector_db_service import vector_db_service

            logger.info(
                "å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“",
                collection=collection_name,
                data_id=data.get("id"),
            )

            # æ ¹æ®é›†åˆç±»å‹è°ƒç”¨ç›¸åº”çš„ç´¢å¼•æ–¹æ³•
            if collection_name == "orders":
                await vector_db_service.index_order(data)
            elif collection_name == "dishes":
                await vector_db_service.index_dish(data)
            elif collection_name == "events":
                await vector_db_service.index_event(data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é›†åˆç±»å‹: {collection_name}")

            logger.info(
                "å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ",
                collection=collection_name,
                data_id=data.get("id"),
            )

            return {
                "success": True,
                "collection": collection_name,
                "data_id": data.get("id"),
            }

        except Exception as e:
            logger.error(
                "å‘é‡æ•°æ®åº“ç´¢å¼•å¤±è´¥",
                collection=collection_name,
                error=str(e),
                exc_info=e,
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
)
def index_order_to_vector_db(
    self,
    order_data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•è®¢å•åˆ°å‘é‡æ•°æ®åº“

    Args:
        order_data: è®¢å•æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        from ..services.vector_db_service import vector_db_service
        from ..services.domain_vector_service import domain_vector_service
        store_id = order_data.get("store_id", "")
        logger.info("å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“", collection="orders", data_id=order_data.get("id"))
        await vector_db_service.index_order(order_data)
        await domain_vector_service.index_revenue_event(store_id, order_data)
        logger.info("å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ", collection="orders/revenue", data_id=order_data.get("id"))
        return {"success": True, "collection": "orders", "data_id": order_data.get("id")}
    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
)
def index_dish_to_vector_db(
    self,
    dish_data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•èœå“åˆ°å‘é‡æ•°æ®åº“

    Args:
        dish_data: èœå“æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        from ..services.vector_db_service import vector_db_service
        from ..services.domain_vector_service import domain_vector_service
        store_id = dish_data.get("store_id", "")
        logger.info("å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“", collection="dishes", data_id=dish_data.get("id"))
        await vector_db_service.index_dish(dish_data)
        await domain_vector_service.index_menu_item(store_id, dish_data)
        logger.info("å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ", collection="dishes/menu", data_id=dish_data.get("id"))
        return {"success": True, "collection": "dishes", "data_id": dish_data.get("id")}
    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
)
def batch_index_orders(
    self,
    orders: list[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    æ‰¹é‡ç´¢å¼•è®¢å•åˆ°å‘é‡æ•°æ®åº“

    Args:
        orders: è®¢å•åˆ—è¡¨

    Returns:
        æ‰¹é‡ç´¢å¼•ç»“æœ
    """
    try:
        logger.info("å¼€å§‹æ‰¹é‡ç´¢å¼•è®¢å•", count=len(orders))

        # ä¸ºæ¯ä¸ªè®¢å•åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [
            index_order_to_vector_db.delay(order)
            for order in orders
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [task.get(timeout=int(os.getenv("CELERY_TASK_GET_TIMEOUT", "300"))) for task in tasks]

        success_count = sum(1 for r in results if r.get("success"))

        logger.info(
            "æ‰¹é‡ç´¢å¼•è®¢å•å®Œæˆ",
            total=len(orders),
            success=success_count,
            failed=len(orders) - success_count,
        )

        return {
            "success": True,
            "total": len(orders),
            "success_count": success_count,
            "failed_count": len(orders) - success_count,
        }

    except Exception as e:
        logger.error("æ‰¹é‡ç´¢å¼•è®¢å•å¤±è´¥", error=str(e), exc_info=e)
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
)
def batch_index_dishes(
    self,
    dishes: list[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    æ‰¹é‡ç´¢å¼•èœå“åˆ°å‘é‡æ•°æ®åº“

    Args:
        dishes: èœå“åˆ—è¡¨

    Returns:
        æ‰¹é‡ç´¢å¼•ç»“æœ
    """
    try:
        logger.info("å¼€å§‹æ‰¹é‡ç´¢å¼•èœå“", count=len(dishes))

        # ä¸ºæ¯ä¸ªèœå“åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [
            index_dish_to_vector_db.delay(dish)
            for dish in dishes
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [task.get(timeout=int(os.getenv("CELERY_TASK_GET_TIMEOUT", "300"))) for task in tasks]

        success_count = sum(1 for r in results if r.get("success"))

        logger.info(
            "æ‰¹é‡ç´¢å¼•èœå“å®Œæˆ",
            total=len(dishes),
            success=success_count,
            failed=len(dishes) - success_count,
        )

        return {
            "success": True,
            "total": len(dishes),
            "success_count": success_count,
            "failed_count": len(dishes) - success_count,
        }

    except Exception as e:
        logger.error("æ‰¹é‡ç´¢å¼•èœå“å¤±è´¥", error=str(e), exc_info=e)
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),  # 5åˆ†é’Ÿ
)
def generate_and_send_daily_report(
    self,
    store_id: str = None,
    report_date: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¹¶å‘é€è¥ä¸šæ—¥æŠ¥

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºä¸ºæ‰€æœ‰é—¨åº—ç”Ÿæˆï¼ŒBeatè°ƒåº¦æ—¶ä½¿ç”¨)
        report_date: æŠ¥å‘Šæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©ï¼‰

    Returns:
        ç”Ÿæˆå’Œå‘é€ç»“æœ
    """
    async def _run():
        try:
            from datetime import date, datetime, timedelta
            from ..services.daily_report_service import daily_report_service
            from ..services.wechat_work_message_service import wechat_work_message_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..core.database import get_db_session
            from sqlalchemy import select

            # è§£ææ—¥æœŸ
            target_date = (
                datetime.strptime(report_date, "%Y-%m-%d").date()
                if report_date
                else date.today() - timedelta(days=1)
            )

            logger.info(
                "å¼€å§‹ç”Ÿæˆè¥ä¸šæ—¥æŠ¥",
                store_id=store_id,
                report_date=str(target_date)
            )

            # è·å–è¦ç”ŸæˆæŠ¥å‘Šçš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                stores = result.scalars().all()

            total_sent = 0
            for store in stores:
                try:
                    # 1. ç”Ÿæˆæ—¥æŠ¥
                    report = await daily_report_service.generate_daily_report(
                        store_id=str(store.id),
                        report_date=target_date
                    )

                    # 2. æ„å»ºæ¨é€æ¶ˆæ¯
                    message = f"""ã€è¥ä¸šæ—¥æŠ¥ã€‘{target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
é—¨åº—ï¼š{store.name}ï¼ˆ{store.id}ï¼‰

{report.summary}

ğŸ“Š è¯¦ç»†æ•°æ®ï¼š
â€¢ è®¢å•æ•°ï¼š{report.order_count}ç¬”
â€¢ å®¢æµé‡ï¼š{report.customer_count}äºº
â€¢ å®¢å•ä»·ï¼šÂ¥{report.avg_order_value / 100:.2f}

ğŸ“ˆ è¿è¥æŒ‡æ ‡ï¼š
â€¢ ä»»åŠ¡å®Œæˆç‡ï¼š{report.task_completion_rate:.1f}%
â€¢ åº“å­˜é¢„è­¦ï¼š{report.inventory_alert_count}ä¸ª
"""

                    if report.highlights:
                        message += "\nâœ¨ ä»Šæ—¥äº®ç‚¹ï¼š\n"
                        for highlight in report.highlights:
                            message += f"â€¢ {highlight}\n"

                    if report.alerts:
                        message += "\nâš ï¸ éœ€è¦å…³æ³¨ï¼š\n"
                        for alert in report.alerts:
                            message += f"â€¢ {alert}\n"

                    # 3. æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜ï¼Œå‘é€æ¨é€
                    async with get_db_session() as session:
                        mgr_result = await session.execute(
                            select(User).where(
                                User.store_id == store.id,
                                User.is_active == True,
                                User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                User.wechat_user_id.isnot(None)
                            )
                        )
                        managers = mgr_result.scalars().all()

                    sent_count = 0
                    for manager in managers:
                        try:
                            send_result = await wechat_work_message_service.send_text_message(
                                user_id=manager.wechat_user_id,
                                content=message
                            )
                            if send_result.get("success"):
                                sent_count += 1
                        except Exception as send_err:
                            logger.error(
                                "å‘é€æ—¥æŠ¥å¤±è´¥",
                                user_id=str(manager.id),
                                error=str(send_err)
                            )

                    # 4. æ ‡è®°ä¸ºå·²å‘é€
                    if sent_count > 0:
                        await daily_report_service.mark_as_sent(report.id)

                    logger.info(
                        "è¥ä¸šæ—¥æŠ¥ç”Ÿæˆå¹¶å‘é€å®Œæˆ",
                        store_id=str(store.id),
                        report_date=str(target_date),
                        sent_count=sent_count
                    )
                    total_sent += sent_count

                except Exception as store_err:
                    logger.error(
                        "é—¨åº—æ—¥æŠ¥ç”Ÿæˆå¤±è´¥",
                        store_id=str(store.id),
                        error=str(store_err)
                    )
                    continue

            logger.info(
                "æ‰€æœ‰é—¨åº—è¥ä¸šæ—¥æŠ¥ç”Ÿæˆå®Œæˆ",
                stores_processed=len(stores),
                total_sent=total_sent,
                report_date=str(target_date)
            )

            return {
                "success": True,
                "stores_processed": len(stores),
                "total_sent": total_sent,
                "report_date": str(target_date),
            }

        except Exception as e:
            logger.error(
                "ç”Ÿæˆè¥ä¸šæ—¥æŠ¥å¤±è´¥",
                store_id=store_id,
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),  # 5åˆ†é’Ÿ
)
def perform_daily_reconciliation(
    self,
    store_id: str,
    reconciliation_date: str = None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ¯æ—¥å¯¹è´¦

    Args:
        store_id: é—¨åº—ID
        reconciliation_date: å¯¹è´¦æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©ï¼‰

    Returns:
        å¯¹è´¦ç»“æœ
    """
    async def _run():
        try:
            from datetime import date, datetime
            from ..services.reconcile_service import reconcile_service

            logger.info(
                "å¼€å§‹æ‰§è¡Œæ¯æ—¥å¯¹è´¦",
                store_id=store_id,
                reconciliation_date=reconciliation_date
            )

            # è§£ææ—¥æœŸ
            if reconciliation_date:
                target_date = datetime.strptime(reconciliation_date, "%Y-%m-%d").date()
            else:
                from datetime import timedelta
                target_date = date.today() - timedelta(days=1)

            # æ‰§è¡Œå¯¹è´¦
            record = await reconcile_service.perform_reconciliation(
                store_id=store_id,
                reconciliation_date=target_date
            )

            logger.info(
                "æ¯æ—¥å¯¹è´¦å®Œæˆ",
                store_id=store_id,
                reconciliation_date=str(target_date),
                status=record.status.value,
                diff_ratio=record.diff_ratio
            )

            return {
                "success": True,
                "store_id": store_id,
                "reconciliation_date": str(target_date),
                "record_id": str(record.id),
                "status": record.status.value,
                "diff_ratio": record.diff_ratio,
                "alert_sent": record.alert_sent
            }

        except Exception as e:
            logger.error(
                "æ‰§è¡Œæ¯æ—¥å¯¹è´¦å¤±è´¥",
                store_id=store_id,
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def detect_revenue_anomaly(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    æ£€æµ‹è¥æ”¶å¼‚å¸¸ (æ¯15åˆ†é’Ÿæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºæ£€æµ‹æ‰€æœ‰é—¨åº—)

    Returns:
        æ£€æµ‹ç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime, timedelta, date
            from ..agents.decision_agent import DecisionAgent
            from ..services.wechat_alert_service import wechat_alert_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..models.order import Order, OrderStatus
            from ..core.database import get_db_session
            from sqlalchemy import select, func

            logger.info(
                "å¼€å§‹æ£€æµ‹è¥æ”¶å¼‚å¸¸",
                store_id=store_id
            )

            decision_agent = DecisionAgent()
            alerts_sent = 0

            # è·å–è¦æ£€æµ‹çš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        now = datetime.now()
                        today_start = datetime.combine(date.today(), datetime.min.time())

                        # å½“å‰è¥æ”¶ï¼šä»Šå¤©åˆ°ç›®å‰ä¸ºæ­¢å·²å®Œæˆ/å·²ä¸Šèœçš„è®¢å•
                        rev_result = await session.execute(
                            select(func.coalesce(func.sum(Order.final_amount), 0)).where(
                                Order.store_id == store.id,
                                Order.order_time >= today_start,
                                Order.order_time <= now,
                                Order.status.in_([OrderStatus.COMPLETED, OrderStatus.SERVED])
                            )
                        )
                        current_revenue = float(rev_result.scalar() or 0) / 100

                        # é¢„æœŸè¥æ”¶ï¼šè¿‡å»4å‘¨åŒæ˜ŸæœŸåŒæ—¶æ®µçš„å¹³å‡å€¼
                        current_elapsed = timedelta(hours=now.hour, minutes=now.minute)
                        expected_samples = []
                        for weeks_ago in range(1, 5):
                            past_date = date.today() - timedelta(weeks=weeks_ago)
                            past_start = datetime.combine(past_date, datetime.min.time())
                            past_end = past_start + current_elapsed
                            past_rev = await session.execute(
                                select(func.coalesce(func.sum(Order.final_amount), 0)).where(
                                    Order.store_id == store.id,
                                    Order.order_time >= past_start,
                                    Order.order_time <= past_end,
                                    Order.status.in_([OrderStatus.COMPLETED, OrderStatus.SERVED])
                                )
                            )
                            val = float(past_rev.scalar() or 0) / 100
                            if val > 0:
                                expected_samples.append(val)

                        if not expected_samples:
                            # æ— å†å²æ•°æ®ï¼Œè·³è¿‡æœ¬é—¨åº—
                            logger.debug("æ— å†å²è¥æ”¶æ•°æ®ï¼Œè·³è¿‡", store_id=str(store.id))
                            continue

                        expected_revenue = sum(expected_samples) / len(expected_samples)

                        # è®¡ç®—åå·®
                        deviation = ((current_revenue - expected_revenue) / expected_revenue) * 100

                        # åªæœ‰åå·®è¶…è¿‡é˜ˆå€¼æ‰å‘Šè­¦
                        if abs(deviation) > float(os.getenv("REVENUE_ANOMALY_THRESHOLD_PERCENT", "15")):
                            # ä½¿ç”¨DecisionAgentåˆ†æ
                            analysis = await decision_agent.analyze_revenue_anomaly(
                                store_id=str(store.id),
                                current_revenue=current_revenue,
                                expected_revenue=expected_revenue,
                                time_period="today"
                            )

                            if analysis["success"]:
                                # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®ID
                                user_result = await session.execute(
                                    select(User).where(
                                        User.store_id == store.id,
                                        User.is_active == True,
                                        User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                        User.wechat_user_id.isnot(None)
                                    )
                                )
                                managers = user_result.scalars().all()
                                recipient_ids = [m.wechat_user_id for m in managers]

                                if recipient_ids:
                                    # ä½¿ç”¨WeChatAlertServiceå‘é€å‘Šè­¦
                                    alert_result = await wechat_alert_service.send_revenue_alert(
                                        store_id=str(store.id),
                                        store_name=store.name,
                                        current_revenue=current_revenue,
                                        expected_revenue=expected_revenue,
                                        deviation=deviation,
                                        analysis=analysis['data']['analysis'],
                                        recipient_ids=recipient_ids
                                    )

                                    if alert_result.get("success"):
                                        alerts_sent += alert_result.get("sent_count", 0)
                                        logger.info(
                                            "è¥æ”¶å¼‚å¸¸å‘Šè­¦å·²å‘é€",
                                            store_id=str(store.id),
                                            deviation=deviation,
                                            sent_count=alert_result.get("sent_count")
                                        )
                                else:
                                    logger.warning(
                                        "æ— å¯ç”¨æ¥æ”¶äºº",
                                        store_id=str(store.id)
                                    )

                    except Exception as e:
                        logger.error(
                            "é—¨åº—è¥æ”¶å¼‚å¸¸æ£€æµ‹å¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "è¥æ”¶å¼‚å¸¸æ£€æµ‹å®Œæˆ",
                stores_checked=len(stores),
                alerts_sent=alerts_sent
            )

            return {
                "success": True,
                "stores_checked": len(stores),
                "alerts_sent": alerts_sent,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "è¥æ”¶å¼‚å¸¸æ£€æµ‹å¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),
)
def generate_daily_report_with_rag(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¹¶å‘é€æ˜¨æ—¥ç®€æŠ¥ (RAGå¢å¼ºç‰ˆï¼Œæ¯å¤©6AMæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºä¸ºæ‰€æœ‰é—¨åº—ç”Ÿæˆ)

    Returns:
        ç”Ÿæˆç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime, date, timedelta
            from ..agents.decision_agent import DecisionAgent
            from ..services.wechat_work_message_service import wechat_work_message_service
            from ..models.store import Store
            from ..core.database import get_db_session
            from sqlalchemy import select

            logger.info(
                "å¼€å§‹ç”Ÿæˆæ˜¨æ—¥ç®€æŠ¥(RAGå¢å¼º)",
                store_id=store_id
            )

            decision_agent = DecisionAgent()
            reports_sent = 0
            yesterday = date.today() - timedelta(days=1)

            # è·å–è¦ç”ŸæˆæŠ¥å‘Šçš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        # ä½¿ç”¨DecisionAgentç”Ÿæˆç»è¥å»ºè®®
                        recommendations = await decision_agent.generate_business_recommendations(
                            store_id=str(store.id),
                            focus_area=None  # å…¨é¢åˆ†æ
                        )

                        if recommendations["success"]:
                            # æ„å»ºç®€æŠ¥æ¶ˆæ¯
                            message = f"""ğŸ“Š æ˜¨æ—¥ç®€æŠ¥ {yesterday.strftime('%Yå¹´%mæœˆ%dæ—¥')}

    é—¨åº—: {store.name}

    AIç»è¥åˆ†æ:
    {recommendations['data']['recommendations']}

    ---
    åŸºäº{recommendations['data']['context_used']}æ¡å†å²æ•°æ®åˆ†æ
    ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    """

                            # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®IDå¹¶å‘é€
                            from ..models.user import User, UserRole
                            user_result = await session.execute(
                                select(User).where(
                                    User.store_id == store.id,
                                    User.is_active == True,
                                    User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                    User.wechat_user_id.isnot(None)
                                )
                            )
                            managers = user_result.scalars().all()
                            sent_count = 0
                            for manager in managers:
                                try:
                                    send_result = await wechat_work_message_service.send_text_message(
                                        user_id=manager.wechat_user_id,
                                        content=message
                                    )
                                    if send_result.get("success"):
                                        sent_count += 1
                                except Exception as send_err:
                                    logger.error(
                                        "å‘é€ç®€æŠ¥å¤±è´¥",
                                        user_id=str(manager.id),
                                        error=str(send_err)
                                    )

                            logger.info(
                                "æ˜¨æ—¥ç®€æŠ¥å·²ç”Ÿæˆå¹¶å‘é€",
                                store_id=str(store.id),
                                sent_count=sent_count
                            )
                            reports_sent += sent_count

                    except Exception as e:
                        logger.error(
                            "é—¨åº—ç®€æŠ¥ç”Ÿæˆå¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "æ˜¨æ—¥ç®€æŠ¥ç”Ÿæˆå®Œæˆ",
                stores_processed=len(stores),
                reports_sent=reports_sent
            )

            return {
                "success": True,
                "stores_processed": len(stores),
                "reports_sent": reports_sent,
                "report_date": str(yesterday),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "æ˜¨æ—¥ç®€æŠ¥ç”Ÿæˆå¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def check_inventory_alert(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    æ£€æŸ¥åº“å­˜é¢„è­¦ (åˆé«˜å³°å‰1å°æ—¶ï¼Œæ¯å¤©10AMæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰é—¨åº—)

    Returns:
        æ£€æŸ¥ç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime
            from ..agents.inventory_agent import InventoryAgent
            from ..services.wechat_alert_service import wechat_alert_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..models.inventory import InventoryItem, InventoryStatus
            from ..core.database import get_db_session
            from sqlalchemy import select

            logger.info(
                "å¼€å§‹æ£€æŸ¥åº“å­˜é¢„è­¦",
                store_id=store_id
            )

            inventory_agent = InventoryAgent()
            alerts_sent = 0

            # è·å–è¦æ£€æŸ¥çš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        # ä»æ•°æ®åº“æŸ¥è¯¢ä½åº“å­˜/ç¼ºè´§åº“å­˜é¡¹
                        inv_result = await session.execute(
                            select(InventoryItem).where(
                                InventoryItem.store_id == store.id,
                                InventoryItem.status.in_([
                                    InventoryStatus.LOW,
                                    InventoryStatus.CRITICAL,
                                    InventoryStatus.OUT_OF_STOCK,
                                ])
                            )
                        )
                        low_stock_items = inv_result.scalars().all()

                        if not low_stock_items:
                            logger.debug("æ— åº“å­˜é¢„è­¦é¡¹", store_id=str(store.id))
                            continue

                        # æ„å»º InventoryAgent æ‰€éœ€çš„ current_inventory å­—å…¸
                        current_inventory = {
                            item.id: item.current_quantity for item in low_stock_items
                        }

                        # ä½¿ç”¨InventoryAgentæ£€æŸ¥ä½åº“å­˜
                        alert_result = await inventory_agent.check_low_stock_alert(
                            store_id=str(store.id),
                            current_inventory=current_inventory,
                            threshold_hours=int(os.getenv("INVENTORY_ALERT_THRESHOLD_HOURS", "4"))  # åˆé«˜å³°å‰Nå°æ—¶é¢„è­¦
                        )

                        if alert_result["success"]:
                            # æ„å»ºé¢„è­¦é¡¹ç›®åˆ—è¡¨ï¼ˆæ¥è‡ªçœŸå®æ•°æ®ï¼‰
                            alert_items = [
                                {
                                    "dish_name": item.name,
                                    "quantity": item.current_quantity,
                                    "unit": item.unit or "",
                                    "min_quantity": item.min_quantity,
                                    "risk": "high" if item.status in (
                                        InventoryStatus.CRITICAL, InventoryStatus.OUT_OF_STOCK
                                    ) else "medium",
                                }
                                for item in low_stock_items
                            ]

                            # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®ID
                            user_result = await session.execute(
                                select(User).where(
                                    User.store_id == store.id,
                                    User.is_active == True,
                                    User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                    User.wechat_user_id.isnot(None)
                                )
                            )
                            managers = user_result.scalars().all()
                            recipient_ids = [m.wechat_user_id for m in managers]

                            if recipient_ids:
                                # ä½¿ç”¨WeChatAlertServiceå‘é€é¢„è­¦
                                send_result = await wechat_alert_service.send_inventory_alert(
                                    store_id=str(store.id),
                                    store_name=store.name,
                                    alert_items=alert_items,
                                    analysis=alert_result['data']['alert'],
                                    recipient_ids=recipient_ids
                                )

                                if send_result.get("success"):
                                    alerts_sent += send_result.get("sent_count", 0)
                                    logger.info(
                                        "åº“å­˜é¢„è­¦å·²å‘é€",
                                        store_id=str(store.id),
                                        sent_count=send_result.get("sent_count")
                                    )
                            else:
                                logger.warning(
                                    "æ— å¯ç”¨æ¥æ”¶äºº",
                                    store_id=str(store.id)
                                )

                    except Exception as e:
                        logger.error(
                            "é—¨åº—åº“å­˜æ£€æŸ¥å¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "åº“å­˜é¢„è­¦æ£€æŸ¥å®Œæˆ",
                stores_checked=len(stores),
                alerts_sent=alerts_sent
            )

            return {
                "success": True,
                "stores_checked": len(stores),
                "alerts_sent": alerts_sent,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "åº“å­˜é¢„è­¦æ£€æŸ¥å¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


# ------------------------------------------------------------------ #
# å¤§æ•°æ®å¼‚æ­¥å¯¼å‡ºä»»åŠ¡                                                    #
# ------------------------------------------------------------------ #

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="async_export_data",
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def async_export_data(self, job_id: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤§æ•°æ®å¯¼å‡ºä»»åŠ¡

    ä»æ•°æ®åº“åˆ†æ‰¹è¯»å–æ•°æ®ï¼Œç”Ÿæˆ CSV/Excel æ–‡ä»¶ï¼Œ
    å¹¶å°†ç»“æœå†™å…¥ä¸´æ—¶ç›®å½•ï¼Œæ›´æ–° ExportJob çŠ¶æ€ã€‚
    """
    import csv
    import tempfile
    from datetime import datetime, date

    async def _run():
        from src.core.database import AsyncSessionLocal
        from src.models.export_job import ExportJob, ExportStatus
        from sqlalchemy import select, and_

        BATCH_SIZE = int(os.getenv("EXPORT_BATCH_SIZE", "1000"))

        async with AsyncSessionLocal() as session:
            job = await session.get(ExportJob, job_id)
            if not job:
                logger.error("å¯¼å‡ºä»»åŠ¡ä¸å­˜åœ¨", job_id=job_id)
                return {"success": False, "error": "job not found"}
            job.status = ExportStatus.RUNNING
            job.celery_task_id = self.request.id
            await session.commit()
            job_type = job.job_type
            fmt = job.format
            params = job.params or {}

        try:
            rows, headers = await _fetch_export_data(job_type, params)

            total = len(rows)
            tmp_dir = os.getenv("EXPORT_TMP_DIR", tempfile.gettempdir())
            os.makedirs(tmp_dir, exist_ok=True)
            filename = f"export_{job_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}.{fmt}"
            file_path = os.path.join(tmp_dir, filename)

            if fmt == "csv":
                file_size = _write_csv(file_path, headers, rows)
            elif fmt == "xlsx":
                file_size = _write_xlsx(file_path, headers, rows)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {fmt}")

            async with AsyncSessionLocal() as session:
                job = await session.get(ExportJob, job_id)
                if job:
                    job.status = ExportStatus.COMPLETED
                    job.progress = 100
                    job.total_rows = total
                    job.processed_rows = total
                    job.file_path = file_path
                    job.file_size_bytes = file_size
                    job.completed_at = datetime.utcnow().isoformat()
                    await session.commit()

            logger.info("å¯¼å‡ºä»»åŠ¡å®Œæˆ", job_id=job_id, total_rows=total)
            return {"success": True, "job_id": job_id, "total_rows": total}

        except Exception as e:
            logger.error("å¯¼å‡ºä»»åŠ¡å¤±è´¥", job_id=job_id, error=str(e))
            async with AsyncSessionLocal() as session:
                job = await session.get(ExportJob, job_id)
                if job:
                    job.status = ExportStatus.FAILED
                    job.error_message = str(e)
                    await session.commit()
            raise self.retry(exc=e)

    async def _fetch_export_data(job_type: str, params: Dict):
        from src.core.database import AsyncSessionLocal
        from sqlalchemy import select, and_
        from datetime import datetime, date

        if job_type == "transactions":
            from src.models.finance import FinancialTransaction
            headers = ["æ—¥æœŸ", "ç±»å‹", "åˆ†ç±»", "å­åˆ†ç±»", "é‡‘é¢(å…ƒ)", "æè¿°", "æ”¯ä»˜æ–¹å¼", "é—¨åº—ID"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("store_id"):
                    conditions.append(FinancialTransaction.store_id == params["store_id"])
                if params.get("transaction_type"):
                    conditions.append(FinancialTransaction.transaction_type == params["transaction_type"])
                if params.get("start_date"):
                    conditions.append(FinancialTransaction.transaction_date >= date.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(FinancialTransaction.transaction_date <= date.fromisoformat(params["end_date"]))
                stmt = select(FinancialTransaction)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(FinancialTransaction.transaction_date.desc())
                result = await session.execute(stmt)
                rows = [
                    [t.transaction_date.isoformat() if t.transaction_date else "",
                     t.transaction_type or "", t.category or "", t.subcategory or "",
                     round((t.amount or 0) / 100, 2), t.description or "",
                     t.payment_method or "", t.store_id or ""]
                    for t in result.scalars().all()
                ]
            return rows, headers

        elif job_type == "audit_logs":
            from src.models.audit_log import AuditLog
            headers = ["æ—¶é—´", "ç”¨æˆ·ID", "ç”¨æˆ·å", "è§’è‰²", "æ“ä½œ", "èµ„æºç±»å‹", "èµ„æºID", "æè¿°", "IP", "çŠ¶æ€", "é—¨åº—ID"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("user_id"):
                    conditions.append(AuditLog.user_id == params["user_id"])
                if params.get("action"):
                    conditions.append(AuditLog.action == params["action"])
                if params.get("store_id"):
                    conditions.append(AuditLog.store_id == params["store_id"])
                if params.get("start_date"):
                    conditions.append(AuditLog.created_at >= datetime.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(AuditLog.created_at <= datetime.fromisoformat(params["end_date"]))
                stmt = select(AuditLog)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(AuditLog.created_at.desc())
                result = await session.execute(stmt)
                rows = [
                    [log.created_at.isoformat() if log.created_at else "",
                     str(log.user_id) if log.user_id else "", log.username or "",
                     log.user_role or "", log.action or "", log.resource_type or "",
                     str(log.resource_id) if log.resource_id else "", log.description or "",
                     log.ip_address or "", log.status or "",
                     str(log.store_id) if log.store_id else ""]
                    for log in result.scalars().all()
                ]
            return rows, headers

        elif job_type == "orders":
            from src.models.order import Order
            headers = ["è®¢å•å·", "çŠ¶æ€", "æ€»é‡‘é¢(å…ƒ)", "æ¡Œå·", "é—¨åº—ID", "ä¸‹å•æ—¶é—´"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("store_id"):
                    conditions.append(Order.store_id == params["store_id"])
                if params.get("status"):
                    conditions.append(Order.status == params["status"])
                if params.get("start_date"):
                    conditions.append(Order.created_at >= datetime.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(Order.created_at <= datetime.fromisoformat(params["end_date"]))
                stmt = select(Order)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(Order.created_at.desc())
                result = await session.execute(stmt)
                rows = [
                    [o.order_number or str(o.id),
                     o.status.value if hasattr(o.status, "value") else str(o.status or ""),
                     round((o.total_amount or 0) / 100, 2),
                     o.table_number or "", o.store_id or "",
                     o.created_at.isoformat() if o.created_at else ""]
                    for o in result.scalars().all()
                ]
            return rows, headers

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {job_type}ï¼Œå¯é€‰: transactions/audit_logs/orders")

    def _write_csv(file_path: str, headers: list, rows: list) -> int:
        with open(file_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(rows)
        return os.path.getsize(file_path)

    def _write_xlsx(file_path: str, headers: list, rows: list) -> int:
        try:
            import openpyxl
            from openpyxl.styles import Font, PatternFill
            from openpyxl.utils import get_column_letter
        except ImportError:
            raise ImportError("è¯·å®‰è£… openpyxl: pip install openpyxl")
        wb = openpyxl.Workbook()
        ws = wb.active
        hf = Font(bold=True, color="FFFFFF")
        hfill = PatternFill(start_color="2F5496", end_color="2F5496", fill_type="solid")
        for ci, h in enumerate(headers, 1):
            c = ws.cell(row=1, column=ci, value=h)
            c.font = hf
            c.fill = hfill
            ws.column_dimensions[get_column_letter(ci)].width = 16
        for ri, row in enumerate(rows, 2):
            for ci, v in enumerate(row, 1):
                ws.cell(row=ri, column=ci, value=v)
        wb.save(file_path)
        return os.path.getsize(file_path)

    return asyncio.run(_run())


# ---------------------------------------------------------------------------
# å¢é‡å¤‡ä»½ä»»åŠ¡
# ---------------------------------------------------------------------------

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="run_backup",
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def run_backup(self, job_id: str) -> Dict[str, Any]:
    """
    æ‰§è¡Œå…¨é‡/å¢é‡å¤‡ä»½ä»»åŠ¡
    - å…¨é‡ï¼šå¯¼å‡ºæ‰€æœ‰æŒ‡å®šè¡¨çš„æ•°æ®ä¸º JSONï¼Œæ‰“åŒ…æˆ tar.gz
    - å¢é‡ï¼šä»…å¯¼å‡º since_timestamp ä¹‹åæœ‰å˜æ›´çš„è¡Œï¼ˆä¾èµ– updated_at å­—æ®µï¼‰
    """
    import hashlib
    import json
    import tarfile
    import tempfile
    from datetime import datetime, timezone

    async def _run():
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from sqlalchemy import text

        db_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@localhost/zhilian")
        backup_dir = os.getenv("BACKUP_TMP_DIR", "/tmp/backups")
        os.makedirs(backup_dir, exist_ok=True)

        engine = create_async_engine(db_url, echo=False)
        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

        async with async_session() as session:
            # è¯»å– BackupJob
            from src.models.backup_job import BackupJob, BackupStatus
            result = await session.execute(
                text("SELECT * FROM backup_jobs WHERE id = :id"),
                {"id": job_id},
            )
            row = result.mappings().first()
            if not row:
                raise ValueError(f"BackupJob {job_id} ä¸å­˜åœ¨")

            backup_type = row["backup_type"]
            since_ts = row["since_timestamp"]
            tables_filter = row["tables"] or []

            # æ ‡è®° RUNNING
            await session.execute(
                text("UPDATE backup_jobs SET status='running', celery_task_id=:tid, updated_at=NOW() WHERE id=:id"),
                {"tid": self.request.id, "id": job_id},
            )
            await session.commit()

        # è·å–æ‰€æœ‰ç”¨æˆ·è¡¨
        async with async_session() as session:
            res = await session.execute(
                text("SELECT tablename FROM pg_tables WHERE schemaname='public' ORDER BY tablename")
            )
            all_tables = [r[0] for r in res.fetchall()]

        target_tables = [t for t in all_tables if not tables_filter or t in tables_filter]
        # æ’é™¤å¤‡ä»½ç›¸å…³è¡¨ï¼Œé¿å…é€’å½’
        target_tables = [t for t in target_tables if t not in ("backup_jobs", "export_jobs")]

        total = len(target_tables)
        row_counts: Dict[str, int] = {}
        tmp_dir = tempfile.mkdtemp(dir=backup_dir)

        try:
            for idx, table in enumerate(target_tables):
                async with async_session() as session:
                    if backup_type == "incremental" and since_ts:
                        # å¢é‡ï¼šåªå– updated_at > since_timestamp çš„è¡Œ
                        try:
                            res = await session.execute(
                                text(f"SELECT * FROM {table} WHERE updated_at > :ts"),
                                {"ts": since_ts},
                            )
                        except Exception:
                            # è¡¨æ²¡æœ‰ updated_at å­—æ®µæ—¶è·³è¿‡
                            row_counts[table] = 0
                            continue
                    else:
                        res = await session.execute(text(f"SELECT * FROM {table}"))

                    cols = list(res.keys())
                    rows_data = [dict(zip(cols, r)) for r in res.fetchall()]

                    # åºåˆ—åŒ–ï¼ˆUUID/datetime è½¬å­—ç¬¦ä¸²ï¼‰
                    def _serialize(v):
                        if hasattr(v, "isoformat"):
                            return v.isoformat()
                        if hasattr(v, "__str__") and not isinstance(v, (int, float, bool, str, type(None))):
                            return str(v)
                        return v

                    rows_data = [{k: _serialize(v) for k, v in r.items()} for r in rows_data]
                    row_counts[table] = len(rows_data)

                    table_file = os.path.join(tmp_dir, f"{table}.json")
                    with open(table_file, "w", encoding="utf-8") as f:
                        json.dump({"table": table, "rows": rows_data}, f, ensure_ascii=False, indent=2)

                # æ›´æ–°è¿›åº¦
                progress = int((idx + 1) / total * 90)
                async with async_session() as session:
                    await session.execute(
                        text("UPDATE backup_jobs SET progress=:p, updated_at=NOW() WHERE id=:id"),
                        {"p": progress, "id": job_id},
                    )
                    await session.commit()

            # æ‰“åŒ… tar.gz
            ts_str = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
            archive_name = f"backup_{backup_type}_{ts_str}_{job_id[:8]}.tar.gz"
            archive_path = os.path.join(backup_dir, archive_name)
            with tarfile.open(archive_path, "w:gz") as tar:
                tar.add(tmp_dir, arcname="backup")

            # è®¡ç®— SHA256
            sha256 = hashlib.sha256()
            with open(archive_path, "rb") as f:
                for chunk in iter(lambda: f.read(65536), b""):
                    sha256.update(chunk)
            checksum = sha256.hexdigest()
            file_size = os.path.getsize(archive_path)
            completed_at = datetime.now(timezone.utc).isoformat()

            async with async_session() as session:
                await session.execute(
                    text(
                        "UPDATE backup_jobs SET status='completed', progress=100, "
                        "file_path=:fp, file_size_bytes=:fs, checksum=:cs, "
                        "row_counts=:rc, completed_at=:ca, updated_at=NOW() WHERE id=:id"
                    ),
                    {
                        "fp": archive_path,
                        "fs": file_size,
                        "cs": checksum,
                        "rc": json.dumps(row_counts),
                        "ca": completed_at,
                        "id": job_id,
                    },
                )
                await session.commit()

            logger.info("å¤‡ä»½ä»»åŠ¡å®Œæˆ", job_id=job_id, archive=archive_path, checksum=checksum)
            return {"job_id": job_id, "file_path": archive_path, "checksum": checksum}

        except Exception as e:
            logger.error("å¤‡ä»½ä»»åŠ¡å¤±è´¥", job_id=job_id, error=str(e))
            async with async_session() as session:
                await session.execute(
                    text("UPDATE backup_jobs SET status='failed', error_message=:err, updated_at=NOW() WHERE id=:id"),
                    {"err": str(e)[:1000], "id": job_id},
                )
                await session.commit()
            raise self.retry(exc=e)

        finally:
            import shutil
            shutil.rmtree(tmp_dir, ignore_errors=True)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),
)
def generate_daily_hub(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆ T+1 ç»è¥ç»Ÿç­¹å¤‡æˆ˜æ¿

    Args:
        store_id: é—¨åº—ID (None è¡¨ç¤ºä¸ºæ‰€æœ‰æ´»è·ƒé—¨åº—ç”Ÿæˆ)

    Returns:
        ç”Ÿæˆç»“æœ
    """
    async def _run():
        from datetime import date, timedelta
        from ..services.daily_hub_service import daily_hub_service
        from ..models.store import Store
        from ..core.database import get_db_session
        from sqlalchemy import select

        target_date = date.today() + timedelta(days=1)

        async with get_db_session() as session:
            if store_id:
                result = await session.execute(
                    select(Store).where(Store.id == store_id, Store.is_active == True)
                )
            else:
                result = await session.execute(
                    select(Store).where(Store.is_active == True)
                )
            stores = result.scalars().all()

        generated = 0
        for store in stores:
            try:
                await daily_hub_service.generate_battle_board(
                    store_id=str(store.id), target_date=target_date
                )
                generated += 1
                logger.info("å¤‡æˆ˜æ¿ç”ŸæˆæˆåŠŸ", store_id=str(store.id), target_date=str(target_date))
            except Exception as e:
                logger.error("å¤‡æˆ˜æ¿ç”Ÿæˆå¤±è´¥", store_id=str(store.id), error=str(e))

        return {"success": True, "generated": generated, "target_date": str(target_date)}

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 3 â€” æŸè€—æ¨ç† / è§„åˆ™è¯„ä¼° / æœ¬ä½“æ—¥åŒæ­¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="tasks.process_waste_event",
    max_retries=3,
    default_retry_delay=30,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=300,
    retry_jitter=True,
)
def process_waste_event(
    self,
    event_id: str,
) -> Dict[str, Any]:
    """
    æŸè€—äº‹ä»¶äº”æ­¥æ¨ç†ï¼ˆå¼‚æ­¥ï¼Œç”± WasteEventService._enqueue_analysis æŠ•é€’ï¼‰

    æ­¥éª¤ï¼š
      1. ä» PostgreSQL åŠ è½½ WasteEvent
      2. è°ƒç”¨ WasteReasoningEngine.infer_root_cause(event_id)
      3. å°†æ ¹å›  / ç½®ä¿¡åº¦ / è¯æ®é“¾å›å†™ PostgreSQL
      4. åŒæ­¥æ¨ç†ç»“è®ºåˆ° Neo4j

    Args:
        event_id: WasteEvent.event_idï¼ˆæ ¼å¼ WE-XXXXXXXXï¼‰

    Returns:
        {"success": True, "event_id": ..., "root_cause": ..., "confidence": ...}
    """
    async def _run():
        from ..core.database import get_db_session
        from ..services.waste_event_service import WasteEventService

        async with get_db_session() as session:
            svc = WasteEventService(session)

            # æ ‡è®°æ¨ç†ä¸­
            from sqlalchemy import update as _update
            from ..models.waste_event import WasteEvent, WasteEventStatus
            await session.execute(
                _update(WasteEvent)
                .where(WasteEvent.event_id == event_id)
                .values(status=WasteEventStatus.ANALYZING)
            )
            await session.commit()

        # è°ƒç”¨æ¨ç†å¼•æ“ï¼ˆåŒæ­¥é©±åŠ¨ï¼Œç‹¬ç«‹ sessionï¼‰
        try:
            from ..ontology.reasoning import WasteReasoningEngine
            engine = WasteReasoningEngine()
            result = engine.infer_root_cause(event_id)
        except Exception as e:
            logger.warning("æ¨ç†å¼•æ“è°ƒç”¨å¤±è´¥", event_id=event_id, error=str(e))
            return {"success": False, "event_id": event_id, "error": str(e)}

        if not result.get("success"):
            return {"success": False, "event_id": event_id, "error": result.get("error")}

        # å†™å›åˆ†æç»“è®º
        async with get_db_session() as session:
            svc = WasteEventService(session)
            await svc.write_back_analysis(
                event_id=event_id,
                root_cause=result.get("root_cause", "unknown"),
                confidence=result.get("confidence", 0.0),
                evidence=result.get("evidence_chain", {}),
                scores=result.get("scores", {}),
            )
            # åŒæ­¥åˆ° Neo4j
            ev = await svc.get_event(event_id)
            if ev:
                await svc._sync_analysis_to_neo4j(ev)
            await session.commit()

        logger.info(
            "æŸè€—æ¨ç†ä»»åŠ¡å®Œæˆ",
            event_id=event_id,
            root_cause=result.get("root_cause"),
            confidence=result.get("confidence"),
        )
        return {
            "success": True,
            "event_id": event_id,
            "root_cause": result.get("root_cause"),
            "confidence": result.get("confidence"),
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# L3 â€” è·¨åº—çŸ¥è¯†èšåˆå¤œé—´ç‰©åŒ–ï¼ˆå‡Œæ™¨ 2:30 è§¦å‘ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="tasks.nightly_cross_store_sync",
    max_retries=2,
    default_retry_delay=300,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=1800,
    retry_jitter=False,
)
def nightly_cross_store_sync(
    self,
    store_ids: list = None,
) -> Dict[str, Any]:
    """
    L3 è·¨åº—çŸ¥è¯†èšåˆå¤œé—´ç‰©åŒ–ä»»åŠ¡ï¼ˆå»ºè®®å‡Œæ™¨ 2:30 è§¦å‘ï¼‰

    æ‰§è¡Œæ­¥éª¤ï¼š
      1. è®¡ç®—ä¸¤ä¸¤é—¨åº—ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå†™å…¥ store_similarity_cache
      2. é‡å»ºåŒä¼´ç»„ store_peer_groupsï¼ˆtier + region åˆ†ç»„ï¼‰
      3. ç‰©åŒ–æ˜¨æ—¥ cross_store_metricsï¼ˆ6 é¡¹æŒ‡æ ‡ Ã— å…¨é—¨åº—ï¼‰
      4. åŒæ­¥ Neo4j å›¾ï¼ˆStore èŠ‚ç‚¹ + SIMILAR_TO / BENCHMARK_OF / SHARES_RECIPE è¾¹ï¼‰

    Args:
        store_ids: æŒ‡å®šé—¨åº—åˆ—è¡¨ï¼ˆNone = å…¨éƒ¨æ´»è·ƒé—¨åº—ï¼‰

    Returns:
        {
          "similarity_pairs":  N,
          "peer_groups":       N,
          "metrics_upserted":  N,
          "graph_synced":      bool,
          "errors":            [...]
        }
    """
    async def _run():
        from ..core.database import get_db_session
        from ..services.cross_store_knowledge_service import CrossStoreKnowledgeService

        errors = []

        async with get_db_session() as session:
            svc = CrossStoreKnowledgeService(session)

            # Step 1: ç›¸ä¼¼åº¦çŸ©é˜µ
            try:
                sim_result = await svc.compute_pairwise_similarity(store_ids=store_ids)
                similarity_pairs = sim_result.get("pairs_computed", 0)
                logger.info("è·¨åº—ç›¸ä¼¼åº¦çŸ©é˜µå·²è®¡ç®—", pairs=similarity_pairs)
            except Exception as e:
                errors.append({"step": "similarity", "error": str(e)})
                similarity_pairs = 0
                logger.error("ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å¤±è´¥", error=str(e))

            # Step 2: åŒä¼´ç»„é‡å»º
            try:
                pg_result = await svc.build_peer_groups(store_ids=store_ids)
                peer_groups = pg_result.get("groups_built", 0)
                logger.info("åŒä¼´ç»„é‡å»ºå®Œæˆ", groups=peer_groups)
            except Exception as e:
                errors.append({"step": "peer_groups", "error": str(e)})
                peer_groups = 0
                logger.error("åŒä¼´ç»„é‡å»ºå¤±è´¥", error=str(e))

            # Step 3: æ—¥ç»´åº¦æŒ‡æ ‡ç‰©åŒ–
            try:
                mat_result = await svc.materialize_metrics(store_ids=store_ids)
                metrics_upserted = mat_result.get("upserted", 0)
                logger.info("è·¨åº—æŒ‡æ ‡ç‰©åŒ–å®Œæˆ", upserted=metrics_upserted)
            except Exception as e:
                errors.append({"step": "materialize", "error": str(e)})
                metrics_upserted = 0
                logger.error("æŒ‡æ ‡ç‰©åŒ–å¤±è´¥", error=str(e))

            await session.commit()

            # Step 4: Neo4j å›¾åŒæ­¥ï¼ˆç‹¬ç«‹å¼‚å¸¸å¤„ç†ï¼Œä¸å½±å“å‰åºç»“æœï¼‰
            graph_synced = False
            try:
                graph_result = await svc.sync_store_graph(store_ids=store_ids)
                graph_synced = not graph_result.get("skipped", False)
                logger.info("Neo4j è·¨åº—å›¾åŒæ­¥å®Œæˆ", result=graph_result)
            except Exception as e:
                errors.append({"step": "graph_sync", "error": str(e)})
                logger.error("Neo4j è·¨åº—å›¾åŒæ­¥å¤±è´¥", error=str(e))

        logger.info(
            "L3 è·¨åº—çŸ¥è¯†èšåˆå¤œé—´ä»»åŠ¡å®Œæˆ",
            similarity_pairs=similarity_pairs,
            peer_groups=peer_groups,
            metrics_upserted=metrics_upserted,
            graph_synced=graph_synced,
            errors=len(errors),
        )
        return {
            "success":          len(errors) == 0,
            "similarity_pairs": similarity_pairs,
            "peer_groups":      peer_groups,
            "metrics_upserted": metrics_upserted,
            "graph_synced":     graph_synced,
            "errors":           errors,
        }

    try:
        return asyncio.run(_run())
    except Exception as nightly_e:
        raise self.retry(exc=nightly_e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="tasks.evaluate_store_rules",
    max_retries=2,
    default_retry_delay=60,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def evaluate_store_rules(
    self,
    store_id: str,
    kpi_context: Dict[str, Any],
    industry_type: str = "general",
) -> Dict[str, Any]:
    """
    å¯¹é—¨åº— KPI ä¸Šä¸‹æ–‡è¿è¡Œæ¨ç†è§„åˆ™åº“ï¼ŒåŒ¹é…è§¦å‘è§„åˆ™å¹¶è‡ªåŠ¨æ¨é€ä¼å¾®å‘Šè­¦

    å·¥ä½œæµç¨‹ï¼š
      1. ä»è§„åˆ™åº“åŠ è½½ ACTIVE è§„åˆ™
      2. å¯¹ kpi_context æ‰§è¡Œè§„åˆ™åŒ¹é…ï¼Œè·å– Top-10 åŒ¹é…è§„åˆ™
      3. å¯¹å‘½ä¸­è§„åˆ™å†™å…¥ RuleExecution æ—¥å¿—
      4. ç½®ä¿¡åº¦ >= 0.70 æ—¶ï¼Œè‡ªåŠ¨åˆ›å»ºä¼å¾® P1/P2 Action
      5. è¡Œä¸šåŸºå‡†å¯¹æ¯”ï¼ˆè‹¥ industry_type é "general"ï¼‰

    Args:
        store_id: é—¨åº—ID
        kpi_context: å½“å‰ KPI æŒ‡æ ‡å­—å…¸ï¼Œå¦‚
            {"waste_rate": 0.18, "labor_cost_ratio": 0.36, ...}
        industry_type: è¡Œä¸šç±»å‹ï¼ˆseafood / hotpot / fastfood / generalï¼‰

    Returns:
        {"matched": [...], "actions_created": N, "store_id": ...}
    """
    async def _run():
        from ..core.database import get_db_session
        from ..services.knowledge_rule_service import KnowledgeRuleService

        matched_rules = []
        actions_created = 0

        async with get_db_session() as session:
            rule_svc = KnowledgeRuleService(session)

            # è§„åˆ™åŒ¹é…ï¼ˆå…¨å“ç±»ï¼‰
            matched = await rule_svc.match_rules(kpi_context)

            for hit in matched:
                matched_rules.append(hit)

                # å†™å…¥æ‰§è¡Œæ—¥å¿—
                rule_obj = await rule_svc.get_by_code(hit["rule_code"])
                if rule_obj:
                    await rule_svc.log_execution(
                        rule=rule_obj,
                        store_id=store_id,
                        event_id=None,
                        condition_values=kpi_context,
                        conclusion_output=hit.get("conclusion", {}),
                        confidence_score=hit["confidence"],
                    )

                # ç½®ä¿¡åº¦ â‰¥ 0.70 â†’ æ¨é€ä¼å¾®å‘Šè­¦
                if hit["confidence"] >= 0.70:
                    try:
                        from ..services.wechat_action_fsm import (
                            ActionCategory,
                            ActionPriority,
                            get_wechat_fsm,
                        )
                        fsm = get_wechat_fsm()
                        priority = (
                            ActionPriority.P1 if hit["confidence"] >= 0.80
                            else ActionPriority.P2
                        )
                        conclusion = hit.get("conclusion", {})
                        action_text = (
                            conclusion.get("action") or
                            conclusion.get("conclusion", "è¯·æ£€æŸ¥ç›¸å…³æŒ‡æ ‡")
                        )
                        await fsm.create_action(
                            store_id=store_id,
                            category=ActionCategory.KPI_ALERT,
                            priority=priority,
                            title=f"è§„åˆ™å‘Šè­¦ï¼š{hit['rule_code']}",
                            content=(
                                f"**{hit['name']}**\n"
                                f"ç½®ä¿¡åº¦ï¼š{hit['confidence']:.0%}\n"
                                f"å»ºè®®ï¼š{action_text}"
                            ),
                            receiver_user_id="store_manager",
                            source_event_id=f"RULE-{hit['rule_code']}-{store_id}",
                            evidence={"kpi_context": kpi_context, "rule_code": hit["rule_code"]},
                        )
                        actions_created += 1
                    except Exception as e:
                        logger.warning(
                            "è§„åˆ™è§¦å‘ä¼å¾®å‘Šè­¦å¤±è´¥",
                            store_id=store_id,
                            rule_code=hit["rule_code"],
                            error=str(e),
                        )

            # è¡Œä¸šåŸºå‡†å¯¹æ¯”ï¼ˆé general æ—¶ï¼‰
            benchmark_summary = []
            if industry_type != "general":
                benchmark_results = await rule_svc.compare_to_benchmark(
                    industry_type, kpi_context
                )
                for br in benchmark_results:
                    if br["percentile_band"] == "bottom_25":
                        benchmark_summary.append(br)

            await session.commit()

        logger.info(
            "é—¨åº—è§„åˆ™è¯„ä¼°å®Œæˆ",
            store_id=store_id,
            matched=len(matched_rules),
            actions_created=actions_created,
        )
        return {
            "success": True,
            "store_id": store_id,
            "matched": matched_rules,
            "actions_created": actions_created,
            "bottom_25_benchmarks": benchmark_summary,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="tasks.daily_ontology_sync",
    max_retries=2,
    default_retry_delay=300,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=1800,
    retry_jitter=False,
)
def daily_ontology_sync(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    æ¯æ—¥å…¨é‡ PostgreSQL â†’ Neo4j æœ¬ä½“åŒæ­¥ï¼ˆå»ºè®®å‡Œæ™¨ 3:00 è§¦å‘ï¼‰

    èŒƒå›´ï¼š
      - æ´»è·ƒ BOMTemplate åŠæ˜ç»†è¡Œ â†’ BOM èŠ‚ç‚¹ + HAS_INGREDIENT è¾¹
      - WasteEventï¼ˆæœ€è¿‘ 30 å¤©ï¼‰â†’ WasteEvent èŠ‚ç‚¹ + WASTE_OF è¾¹

    Args:
        store_id: æŒ‡å®šé—¨åº—ï¼ˆNone = å…¨éƒ¨æ´»è·ƒé—¨åº—ï¼‰

    Returns:
        {"synced_stores": N, "nodes_upserted": N, "errors": [...]}
    """
    async def _run():
        from ..core.database import get_db_session
        from ..models.store import Store
        from ..models.bom import BOMTemplate
        from ..models.waste_event import WasteEvent
        from sqlalchemy import select, and_
        from datetime import datetime, timedelta

        synced_stores = 0
        nodes_upserted = 0
        errors = []

        async with get_db_session() as session:
            # ç¡®å®šè¦åŒæ­¥çš„é—¨åº—
            if store_id:
                stmt = select(Store).where(Store.id == store_id, Store.is_active.is_(True))
            else:
                stmt = select(Store).where(Store.is_active.is_(True))
            result = await session.execute(stmt)
            stores = result.scalars().all()

            for store in stores:
                sid = str(store.id)
                try:
                    # 1. åŒæ­¥æ´»è·ƒ BOM
                    bom_stmt = (
                        select(BOMTemplate)
                        .where(
                            and_(BOMTemplate.store_id == sid, BOMTemplate.is_active.is_(True))
                        )
                    )
                    bom_result = await session.execute(bom_stmt)
                    active_boms = bom_result.scalars().all()

                    try:
                        from ..ontology.data_sync import OntologyDataSync
                        with OntologyDataSync() as sync:
                            for bom in active_boms:
                                dish_id_str = f"DISH-{bom.dish_id}"
                                sync.upsert_bom(
                                    dish_id=dish_id_str,
                                    version=bom.version,
                                    effective_date=bom.effective_date,
                                    yield_rate=float(bom.yield_rate),
                                    expiry_date=bom.expiry_date,
                                    notes=bom.notes,
                                )
                                nodes_upserted += 1
                    except Exception as neo4j_err:
                        logger.warning("Neo4j BOM åŒæ­¥å¤±è´¥", store_id=sid, error=str(neo4j_err))

                    # 2. åŒæ­¥è¿‘ 30 å¤© WasteEvent
                    since = datetime.utcnow() - timedelta(days=30)
                    we_stmt = select(WasteEvent).where(
                        and_(
                            WasteEvent.store_id == sid,
                            WasteEvent.occurred_at >= since,
                        )
                    )
                    we_result = await session.execute(we_stmt)
                    events = we_result.scalars().all()

                    from ..services.waste_event_service import WasteEventService
                    svc = WasteEventService(session)
                    for ev in events:
                        try:
                            await svc._sync_to_neo4j(ev)
                            nodes_upserted += 1
                        except Exception as e:
                            logger.warning(
                                "WasteEvent Neo4j åŒæ­¥å¤±è´¥",
                                event_id=ev.event_id,
                                error=str(e),
                            )

                    synced_stores += 1
                    logger.info(
                        "é—¨åº—æœ¬ä½“åŒæ­¥å®Œæˆ",
                        store_id=sid,
                        boms=len(active_boms),
                        waste_events=len(events),
                    )

                except Exception as e:
                    errors.append({"store_id": sid, "error": str(e)})
                    logger.error("é—¨åº—æœ¬ä½“åŒæ­¥å¤±è´¥", store_id=sid, error=str(e))

        logger.info(
            "æ—¥å¸¸æœ¬ä½“åŒæ­¥å®Œæˆ",
            synced_stores=synced_stores,
            nodes_upserted=nodes_upserted,
            errors=len(errors),
        )
        return {
            "success": True,
            "synced_stores": synced_stores,
            "nodes_upserted": nodes_upserted,
            "errors": errors,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# L4 â€” å…¨å¹³å°æ¨ç†æ‰«æå¤œé—´ä»»åŠ¡ï¼ˆå‡Œæ™¨ 3:30 è§¦å‘ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="tasks.nightly_reasoning_scan",
    max_retries=2,
    default_retry_delay=300,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=1800,
    retry_jitter=False,
)
def nightly_reasoning_scan(
    self,
    store_ids: list = None,
) -> Dict[str, Any]:
    """
    L4 å…¨å¹³å°æ¨ç†æ‰«æå¤œé—´ä»»åŠ¡ï¼ˆå»ºè®®å‡Œæ™¨ 3:30 è§¦å‘ï¼Œåœ¨ L3 nightly_cross_store_sync ä¹‹åï¼‰

    æ‰§è¡Œæ­¥éª¤ï¼š
      1. ä» PostgreSQL æ‹‰å–æ‰€æœ‰æ´»è·ƒé—¨åº—æœ€è¿‘ 24h KPI å¿«ç…§
      2. è°ƒç”¨ DiagnosisService.run_full_diagnosis() å…¨ç»´åº¦æ¨ç†
      3. ç»“è®ºå†™å…¥ reasoning_reportsï¼ˆupsertï¼Œå¹‚ç­‰ï¼‰
      4. å¯¹ P1/P2 æŠ¥å‘ŠåŒæ­¥å†™å…¥ Neo4j ReasoningReport èŠ‚ç‚¹
      5. ç½®ä¿¡åº¦ â‰¥ 0.70 çš„ P1/P2 å‘Šè­¦è‡ªåŠ¨æ¨é€ä¼å¾®

    Args:
        store_ids: æŒ‡å®šé—¨åº—åˆ—è¡¨ï¼ˆNone = å…¨éƒ¨æ´»è·ƒé—¨åº—ï¼‰

    Returns:
        {
          "stores_scanned":  N,
          "p1_alerts":       N,
          "p2_alerts":       N,
          "wechat_sent":     N,
          "errors":          [...]
        }
    """
    async def _run():
        from ..core.database import get_db_session
        from ..models.store import Store
        from ..services.diagnosis_service import DiagnosisService
        from ..services.reasoning_engine import ALL_DIMENSIONS
        from ..models.reasoning import ReasoningReport
        from sqlalchemy import select, and_, func

        errors = []
        stores_scanned = 0
        p1_count = 0
        p2_count = 0
        wechat_sent = 0

        async with get_db_session() as session:
            # æ‹‰å–æ´»è·ƒé—¨åº—åˆ—è¡¨
            if store_ids:
                stmt = select(Store).where(
                    and_(Store.id.in_(store_ids), Store.is_active.is_(True))
                )
            else:
                stmt = select(Store).where(Store.is_active.is_(True))
            stores = (await session.execute(stmt)).scalars().all()

            svc = DiagnosisService(session)

            for store in stores:
                sid = str(store.id)
                try:
                    # æ„é€ é—¨åº— KPI å¿«ç…§ï¼ˆä»è¿‘ 30 å¤© cross_store_metrics ç‰©åŒ–æ•°æ®ï¼‰
                    kpi_context = await _build_kpi_context(session, sid)
                    if not kpi_context:
                        logger.debug("é—¨åº—æ—  KPI æ•°æ®ï¼Œè·³è¿‡æ¨ç†", store_id=sid)
                        continue

                    # å…¨ç»´åº¦è¯Šæ–­
                    report = await svc.run_full_diagnosis(
                        store_id=sid,
                        kpi_context=kpi_context,
                    )
                    stores_scanned += 1

                    # ç»Ÿè®¡ P1/P2
                    for dim, c in report.dimensions.items():
                        if c.severity == "P1":
                            p1_count += 1
                        elif c.severity == "P2":
                            p2_count += 1

                    # P1/P2 â†’ åŒæ­¥ Neo4j + ä¼å¾®æ¨é€
                    for dim, c in report.dimensions.items():
                        if c.severity in ("P1", "P2") and c.confidence >= 0.70:
                            # Neo4j åŒæ­¥
                            try:
                                from ..ontology.data_sync import OntologyDataSync
                                import uuid as _uuid
                                # æŸ¥æ‰¾åˆšå†™å…¥çš„ reasoning_report id
                                from ..models.reasoning import ReasoningReport as RR
                                from datetime import date
                                rr_stmt = select(RR).where(
                                    and_(
                                        RR.store_id    == sid,
                                        RR.report_date == date.today(),
                                        RR.dimension   == dim,
                                    )
                                ).limit(1)
                                rr = (await session.execute(rr_stmt)).scalar_one_or_none()
                                if rr:
                                    with OntologyDataSync() as sync:
                                        sync.upsert_reasoning_report(
                                            report_id=str(rr.id),
                                            store_id=sid,
                                            report_date=rr.report_date.isoformat(),
                                            dimension=dim,
                                            severity=c.severity,
                                            root_cause=c.root_cause,
                                            confidence=c.confidence,
                                            triggered_rules=c.triggered_rules,
                                        )
                            except Exception as neo4j_err:
                                logger.warning(
                                    "Neo4j ReasoningReport åŒæ­¥å¤±è´¥",
                                    store_id=sid,
                                    error=str(neo4j_err),
                                )

                            # ä¼å¾®æ¨é€
                            try:
                                from ..services.wechat_action_fsm import (
                                    ActionCategory,
                                    ActionPriority,
                                    get_wechat_fsm,
                                )
                                fsm      = get_wechat_fsm()
                                priority = (
                                    ActionPriority.P1
                                    if c.severity == "P1"
                                    else ActionPriority.P2
                                )
                                action_text = (
                                    c.recommended_actions[0]
                                    if c.recommended_actions
                                    else "è¯·æŸ¥çœ‹æ¨ç†æŠ¥å‘Šå¹¶é‡‡å–è¡ŒåŠ¨"
                                )
                                await fsm.create_action(
                                    store_id=sid,
                                    category=ActionCategory.KPI_ALERT,
                                    priority=priority,
                                    title=f"L4æ¨ç†å‘Šè­¦ï¼š{dim} ç»´åº¦ {c.severity}",
                                    content=(
                                        f"**{dim}** ç»´åº¦å¼‚å¸¸\n"
                                        f"æ ¹å› : {c.root_cause or 'å¾…åˆ†æ'}\n"
                                        f"ç½®ä¿¡åº¦: {c.confidence:.0%}\n"
                                        f"å»ºè®®: {action_text}"
                                    ),
                                    receiver_user_id="store_manager",
                                    source_event_id=f"L4-{sid}-{dim}",
                                    evidence={"dimension": dim, "severity": c.severity},
                                )
                                wechat_sent += 1
                            except Exception as wx_err:
                                logger.warning(
                                    "L4 ä¼å¾®å‘Šè­¦æ¨é€å¤±è´¥",
                                    store_id=sid,
                                    dim=dim,
                                    error=str(wx_err),
                                )

                except Exception as e:
                    errors.append({"store_id": sid, "error": str(e)})
                    logger.error("é—¨åº—æ¨ç†æ‰«æå¤±è´¥", store_id=sid, error=str(e))

            await session.commit()

        logger.info(
            "L4 å¤œé—´æ¨ç†æ‰«æå®Œæˆ",
            stores_scanned=stores_scanned,
            p1_alerts=p1_count,
            p2_alerts=p2_count,
            wechat_sent=wechat_sent,
            errors=len(errors),
        )
        return {
            "success":       len(errors) == 0,
            "stores_scanned": stores_scanned,
            "p1_alerts":     p1_count,
            "p2_alerts":     p2_count,
            "wechat_sent":   wechat_sent,
            "errors":        errors,
        }

    async def _build_kpi_context(session, store_id: str) -> Dict[str, Any]:
        """ä» cross_store_metrics ç‰©åŒ–è¡¨æ‹‰å–è¿‘æœŸ KPI å€¼"""
        from ..models.cross_store import CrossStoreMetric
        from datetime import date, timedelta
        from sqlalchemy import select, and_

        yesterday = date.today() - timedelta(days=1)
        stmt = select(
            CrossStoreMetric.metric_name,
            CrossStoreMetric.value,
        ).where(
            and_(
                CrossStoreMetric.store_id    == store_id,
                CrossStoreMetric.metric_date == yesterday,
            )
        )
        rows = (await session.execute(stmt)).all()
        return {r[0]: r[1] for r in rows}

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


# â”€â”€ L5 å¤œé—´è¡ŒåŠ¨æ´¾å‘ä»»åŠ¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@celery_app.task(
    base=CallbackTask, bind=True,
    name="tasks.nightly_action_dispatch",
    max_retries=2, default_retry_delay=300,
    autoretry_for=(Exception,), retry_backoff=True,
    retry_backoff_max=1800, retry_jitter=False,
)
def nightly_action_dispatch(
    self,
    store_ids: list = None,
    days_back: int  = 1,
) -> Dict[str, Any]:
    """
    L5 å¤œé—´è¡ŒåŠ¨æ‰¹é‡æ´¾å‘ä»»åŠ¡ï¼ˆå»ºè®®è°ƒåº¦æ—¶é—´: 04:30ï¼ŒL4 nightly_reasoning_scan å®Œæˆåæ‰§è¡Œï¼‰

    æ‰§è¡Œæ­¥éª¤ï¼š
      1. æŸ¥è¯¢è¿‘ days_back å¤©å†…æ‰€æœ‰æœªæ´¾å‘è¡ŒåŠ¨çš„ P1/P2 æ¨ç†æŠ¥å‘Š
      2. æŒ‰ severity ä¼˜å…ˆçº§ï¼ˆP1 ä¼˜å…ˆï¼‰é€ä¸€è§¦å‘ ActionDispatchService.dispatch_from_report()
      3. æ±‡æ€»æ´¾å‘ç»Ÿè®¡å¹¶è¿”å›

    Args:
        store_ids: æŒ‡å®šé—¨åº—åˆ—è¡¨ï¼ˆNone = å…¨å¹³å°ï¼‰
        days_back: å›æº¯å¤©æ•°ï¼ˆé»˜è®¤ 1 = ä»…å¤„ç†æ˜¨æ—¥å’Œä»Šæ—¥æŠ¥å‘Šï¼‰

    Returns:
        {success, stores_covered, plans_created, dispatched, partial, skipped, errors}
    """
    import asyncio
    from typing import Dict, Any

    async def _run():
        from src.core.database import async_session_factory
        from src.services.action_dispatch_service import ActionDispatchService
        from src.models.reasoning import ReasoningReport
        from src.models.action_plan import ActionPlan
        from datetime import date, timedelta
        from sqlalchemy import select, and_

        total_stats = {
            "plans_created": 0, "dispatched": 0,
            "partial": 0, "skipped": 0, "errors": 0,
        }
        stores_covered: set = set()

        async with async_session_factory() as session:
            # ç¡®å®šç›®æ ‡é—¨åº—
            if store_ids:
                target_stores = store_ids
            else:
                from src.models.store import Store
                rows  = (await session.execute(
                    select(Store.id).where(Store.is_active == True)  # noqa: E712
                )).all()
                target_stores = [r[0] for r in rows]

            svc = ActionDispatchService(session)
            for sid in target_stores:
                try:
                    stats = await svc.dispatch_pending_alerts(
                        store_id=sid, days_back=days_back
                    )
                    if stats["plans_created"] > 0 or stats["dispatched"] > 0:
                        stores_covered.add(sid)
                    for k in ("plans_created", "dispatched", "partial", "skipped", "errors"):
                        total_stats[k] += stats.get(k, 0)
                except Exception as e:
                    total_stats["errors"] += 1
                    logger.error("L5 é—¨åº—è¡ŒåŠ¨æ´¾å‘å¤±è´¥", store_id=sid, error=str(e))

            await session.commit()

        logger.info(
            "L5 å¤œé—´è¡ŒåŠ¨æ´¾å‘å®Œæˆ",
            stores_covered=len(stores_covered),
            **total_stats,
        )
        return {
            "success":         total_stats["errors"] == 0,
            "stores_covered":  len(stores_covered),
            **total_stats,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


# â”€â”€ å¤šé˜¶æ®µå·¥ä½œæµ Celery ä»»åŠ¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@celery_app.task(
    base=CallbackTask, bind=True,
    name="tasks.start_evening_planning_all_stores",
    max_retries=2, default_retry_delay=120,
    autoretry_for=(Exception,), retry_backoff=True,
    retry_backoff_max=600, retry_jitter=True,
)
def start_evening_planning_all_stores(
    self,
    store_ids: list = None,
) -> Dict[str, Any]:
    """
    æ¯æ—¥ 17:00 è§¦å‘ï¼šä¸ºå…¨å¹³å°æ‰€æœ‰æ´»è·ƒé—¨åº—å¯åŠ¨ Day N+1 è§„åˆ’å·¥ä½œæµï¼Œ
    å¹¶ç«‹å³è§¦å‘ initial_plan é˜¶æ®µçš„å¿«é€Ÿè§„åˆ’ï¼ˆFast Modeï¼Œ<30sï¼‰ã€‚

    è°ƒåº¦å»ºè®®ï¼šbeat_schedule ä¸­è®¾ç½® crontab(hour=17, minute=0)

    Args:
        store_ids: æŒ‡å®šé—¨åº—åˆ—è¡¨ï¼ˆNone = å…¨å¹³å°æ´»è·ƒé—¨åº—ï¼‰

    Returns:
        {success, stores_started, fast_plan_ok, fast_plan_failed, errors}
    """
    async def _run():
        from datetime import date, timedelta
        from src.core.database import async_session_factory
        from src.services.workflow_engine import WorkflowEngine
        from src.services.fast_planning_service import FastPlanningService

        plan_date      = date.today() + timedelta(days=1)
        started        = 0
        fast_plan_ok   = 0
        fast_plan_fail = 0
        errors         = 0

        async with async_session_factory() as session:
            # ç¡®å®šç›®æ ‡é—¨åº—
            if store_ids:
                target_stores = store_ids
            else:
                from src.models.store import Store
                from sqlalchemy import select
                rows = (await session.execute(
                    select(Store.id).where(Store.is_active == True)  # noqa: E712
                )).all()
                target_stores = [str(r[0]) for r in rows]

            engine   = WorkflowEngine(session)
            fast_svc = FastPlanningService(session)

            for sid in target_stores:
                try:
                    # 1. å¯åŠ¨å·¥ä½œæµï¼ˆå¹‚ç­‰ï¼‰
                    wf = await engine.start_daily_workflow(
                        store_id=sid,
                        plan_date=plan_date,
                    )
                    started += 1

                    # 2. è·å– initial_plan é˜¶æ®µï¼Œè§¦å‘å¿«é€Ÿè§„åˆ’
                    try:
                        init_phase = await engine.get_phase(wf.id, "initial_plan")
                        if init_phase:
                            content = await fast_svc.generate_initial_plan(sid, plan_date)
                            await engine.submit_decision(
                                phase_id=init_phase.id,
                                content=content,
                                submitted_by="system",
                                mode="fast",
                                data_completeness=content.get("data_completeness", 0.7),
                                confidence=content.get("confidence", 0.75),
                                change_reason="ç³»ç»Ÿ 17:00 å¿«é€Ÿè§„åˆ’è‡ªåŠ¨ç”Ÿæˆ",
                            )
                            fast_plan_ok += 1
                    except Exception as fp_err:
                        fast_plan_fail += 1
                        logger.warning(
                            "å¿«é€Ÿè§„åˆ’è§¦å‘å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰",
                            store_id=sid,
                            error=str(fp_err),
                        )

                except Exception as e:
                    errors += 1
                    logger.error("å¯åŠ¨å·¥ä½œæµå¤±è´¥", store_id=sid, error=str(e))

            await session.commit()

        logger.info(
            "æ™šé—´è§„åˆ’å·¥ä½œæµæ‰¹é‡å¯åŠ¨å®Œæˆ",
            plan_date=str(plan_date),
            stores_started=started,
            fast_plan_ok=fast_plan_ok,
            fast_plan_failed=fast_plan_fail,
            errors=errors,
        )
        return {
            "success":          errors == 0,
            "plan_date":        str(plan_date),
            "stores_started":   started,
            "fast_plan_ok":     fast_plan_ok,
            "fast_plan_failed": fast_plan_fail,
            "errors":           errors,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask, bind=True,
    name="tasks.check_workflow_deadlines",
    max_retries=1, default_retry_delay=30,
)
def check_workflow_deadlines(self) -> Dict[str, Any]:
    """
    æ¯ 5 åˆ†é’Ÿæ‰«æå…¨å¹³å°æ‰€æœ‰ running/reviewing å·¥ä½œæµé˜¶æ®µï¼š
      - è· deadline â‰¤ 10 åˆ†é’Ÿ â†’ å‘é€ä¼å¾®é¢„è­¦
      - å·²è¿‡ deadline          â†’ è‡ªåŠ¨é”å®šé˜¶æ®µå¹¶æ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ

    è°ƒåº¦å»ºè®®ï¼šbeat_schedule ä¸­è®¾ç½® crontab(minute="*/5")

    Returns:
        {success, auto_locked, locked_phases}
    """
    async def _run():
        from src.core.database import async_session_factory
        from src.services.timing_service import TimingService

        async with async_session_factory() as session:
            timing = TimingService(session)
            locked = await timing.check_and_auto_lock_all()
            await session.commit()

        return {
            "success":       True,
            "auto_locked":   len(locked),
            "locked_phases": locked,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)
