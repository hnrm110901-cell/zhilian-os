"""
Celeryå¼‚æ­¥ä»»åŠ¡
ç”¨äºNeural Systemçš„äº‹ä»¶å¤„ç†å’Œå‘é‡æ•°æ®åº“ç´¢å¼•
"""
from typing import Dict, Any
import asyncio
import os
import structlog
from celery import Task

from .celery_app import celery_app

logger = structlog.get_logger()


class CallbackTask(Task):
    """å¸¦å›è°ƒçš„ä»»åŠ¡åŸºç±»"""

    def on_success(self, retval, task_id, args, kwargs):
        """ä»»åŠ¡æˆåŠŸå›è°ƒ"""
        logger.info(
            "Celeryä»»åŠ¡æˆåŠŸ",
            task_id=task_id,
            task_name=self.name,
            result=retval,
        )

    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥å›è°ƒ"""
        logger.error(
            "Celeryä»»åŠ¡å¤±è´¥",
            task_id=task_id,
            task_name=self.name,
            error=str(exc),
            traceback=str(einfo),
        )

    def on_retry(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡é‡è¯•å›è°ƒ"""
        logger.warning(
            "Celeryä»»åŠ¡é‡è¯•",
            task_id=task_id,
            task_name=self.name,
            error=str(exc),
            retry_count=self.request.retries,
        )


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=int(os.getenv("CELERY_RETRY_BACKOFF_MAX", "600")),
    retry_jitter=True,
)
def process_neural_event(
    self,
    event_id: str,
    event_type: str,
    event_source: str,
    store_id: str,
    data: Dict[str, Any],
    priority: int = 0,
) -> Dict[str, Any]:
    """
    å¤„ç†ç¥ç»ç³»ç»Ÿäº‹ä»¶ï¼ˆå¼‚æ­¥ä»»åŠ¡ï¼‰

    Args:
        event_id: äº‹ä»¶ID
        event_type: äº‹ä»¶ç±»å‹
        event_source: äº‹ä»¶æ¥æº
        store_id: é—¨åº—ID
        data: äº‹ä»¶æ•°æ®
        priority: ä¼˜å…ˆçº§

    Returns:
        å¤„ç†ç»“æœ
    """
    async def _run():
        from datetime import datetime
        from ..services.vector_db_service import vector_db_service
        from ..core.database import AsyncSessionLocal
        from ..models.neural_event_log import NeuralEventLog, EventProcessingStatus

        # 1. å†™å…¥ DB â€” æ ‡è®°ä¸º processing
        async with AsyncSessionLocal() as session:
            log = NeuralEventLog(
                event_id=event_id,
                celery_task_id=self.request.id,
                event_type=event_type,
                event_source=event_source,
                store_id=store_id,
                priority=priority,
                data=data,
                processing_status=EventProcessingStatus.PROCESSING,
                queued_at=datetime.utcnow(),
                started_at=datetime.utcnow(),
            )
            session.add(log)
            await session.commit()

        logger.info(
            "å¼€å§‹å¤„ç†ç¥ç»ç³»ç»Ÿäº‹ä»¶",
            event_id=event_id,
            event_type=event_type,
            store_id=store_id,
        )

        actions_taken = []
        downstream_tasks = []
        vector_indexed = False
        wechat_sent = False

        try:
            # 2. å‘é‡åŒ–å­˜å‚¨ï¼ˆå…¨å±€ç´¢å¼• + é¢†åŸŸåˆ†å‰²ç´¢å¼•ï¼‰
            event_payload = {
                "event_id": event_id,
                "event_type": event_type,
                "event_source": event_source,
                "timestamp": datetime.utcnow(),
                "store_id": store_id,
                "data": data,
                "priority": priority,
            }
            await vector_db_service.index_event(event_payload)
            from ..services.domain_vector_service import domain_vector_service
            await domain_vector_service.index_neural_event(store_id, event_payload)
            vector_indexed = True
            actions_taken.append("vector_indexed")

            # 3. è§¦å‘ä¼å¾®æ¨é€
            from ..services.wechat_trigger_service import wechat_trigger_service
            try:
                await wechat_trigger_service.trigger_push(
                    event_type=event_type,
                    event_data=data,
                    store_id=store_id,
                )
                wechat_sent = True
                actions_taken.append("wechat_sent")
            except Exception as e:
                logger.warning("ä¼å¾®æ¨é€è§¦å‘å¤±è´¥", event_type=event_type, error=str(e))

            # 4. æ ¹æ®äº‹ä»¶ç±»å‹è§¦å‘ä¸‹æ¸¸ä»»åŠ¡
            if event_type.startswith("order."):
                t = index_order_to_vector_db.delay(data)
                downstream_tasks.append({"task_name": "index_order_to_vector_db", "task_id": t.id})
                actions_taken.append("dispatched:index_order_to_vector_db")
            elif event_type.startswith("dish."):
                t = index_dish_to_vector_db.delay(data)
                downstream_tasks.append({"task_name": "index_dish_to_vector_db", "task_id": t.id})
                actions_taken.append("dispatched:index_dish_to_vector_db")

            processed_at = datetime.utcnow()

            # 5. å†™å› DB â€” æ ‡è®°ä¸º completed
            async with AsyncSessionLocal() as session:
                db_log = await session.get(NeuralEventLog, event_id)
                if db_log:
                    db_log.processing_status = EventProcessingStatus.COMPLETED
                    db_log.vector_indexed = vector_indexed
                    db_log.wechat_sent = wechat_sent
                    db_log.downstream_tasks = downstream_tasks
                    db_log.actions_taken = actions_taken
                    db_log.processed_at = processed_at
                    await session.commit()

            logger.info("ç¥ç»ç³»ç»Ÿäº‹ä»¶å¤„ç†å®Œæˆ", event_id=event_id, event_type=event_type)
            return {
                "success": True,
                "event_id": event_id,
                "processed_at": processed_at.isoformat(),
                "actions_taken": actions_taken,
            }

        except Exception as e:
            logger.error("ç¥ç»ç³»ç»Ÿäº‹ä»¶å¤„ç†å¤±è´¥", event_id=event_id, error=str(e), exc_info=e)
            # å†™å› DB â€” æ ‡è®°ä¸º failed / retrying
            try:
                is_last_retry = self.request.retries >= self.max_retries
                async with AsyncSessionLocal() as session:
                    db_log = await session.get(NeuralEventLog, event_id)
                    if db_log:
                        db_log.processing_status = (
                            EventProcessingStatus.FAILED if is_last_retry
                            else EventProcessingStatus.RETRYING
                        )
                        db_log.error_message = str(e)
                        db_log.retry_count = self.request.retries + 1
                        await session.commit()
            except Exception as db_err:
                logger.warning("celery_tasks.status_update_failed", error=str(db_err))
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_SHORT", "30")),
)
def index_to_vector_db(
    self,
    collection_name: str,
    data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•æ•°æ®åˆ°å‘é‡æ•°æ®åº“ï¼ˆé€šç”¨ä»»åŠ¡ï¼‰

    Args:
        collection_name: é›†åˆåç§°
        data: è¦ç´¢å¼•çš„æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        try:
            from ..services.vector_db_service import vector_db_service

            logger.info(
                "å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“",
                collection=collection_name,
                data_id=data.get("id"),
            )

            # æ ¹æ®é›†åˆç±»å‹è°ƒç”¨ç›¸åº”çš„ç´¢å¼•æ–¹æ³•
            if collection_name == "orders":
                await vector_db_service.index_order(data)
            elif collection_name == "dishes":
                await vector_db_service.index_dish(data)
            elif collection_name == "events":
                await vector_db_service.index_event(data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é›†åˆç±»å‹: {collection_name}")

            logger.info(
                "å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ",
                collection=collection_name,
                data_id=data.get("id"),
            )

            return {
                "success": True,
                "collection": collection_name,
                "data_id": data.get("id"),
            }

        except Exception as e:
            logger.error(
                "å‘é‡æ•°æ®åº“ç´¢å¼•å¤±è´¥",
                collection=collection_name,
                error=str(e),
                exc_info=e,
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
)
def index_order_to_vector_db(
    self,
    order_data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•è®¢å•åˆ°å‘é‡æ•°æ®åº“

    Args:
        order_data: è®¢å•æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        from ..services.vector_db_service import vector_db_service
        from ..services.domain_vector_service import domain_vector_service
        store_id = order_data.get("store_id", "")
        logger.info("å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“", collection="orders", data_id=order_data.get("id"))
        await vector_db_service.index_order(order_data)
        await domain_vector_service.index_revenue_event(store_id, order_data)
        logger.info("å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ", collection="orders/revenue", data_id=order_data.get("id"))
        return {"success": True, "collection": "orders", "data_id": order_data.get("id")}
    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
)
def index_dish_to_vector_db(
    self,
    dish_data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    ç´¢å¼•èœå“åˆ°å‘é‡æ•°æ®åº“

    Args:
        dish_data: èœå“æ•°æ®

    Returns:
        ç´¢å¼•ç»“æœ
    """
    async def _run():
        from ..services.vector_db_service import vector_db_service
        from ..services.domain_vector_service import domain_vector_service
        store_id = dish_data.get("store_id", "")
        logger.info("å¼€å§‹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“", collection="dishes", data_id=dish_data.get("id"))
        await vector_db_service.index_dish(dish_data)
        await domain_vector_service.index_menu_item(store_id, dish_data)
        logger.info("å‘é‡æ•°æ®åº“ç´¢å¼•å®Œæˆ", collection="dishes/menu", data_id=dish_data.get("id"))
        return {"success": True, "collection": "dishes", "data_id": dish_data.get("id")}
    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
)
def batch_index_orders(
    self,
    orders: list[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    æ‰¹é‡ç´¢å¼•è®¢å•åˆ°å‘é‡æ•°æ®åº“

    Args:
        orders: è®¢å•åˆ—è¡¨

    Returns:
        æ‰¹é‡ç´¢å¼•ç»“æœ
    """
    try:
        logger.info("å¼€å§‹æ‰¹é‡ç´¢å¼•è®¢å•", count=len(orders))

        # ä¸ºæ¯ä¸ªè®¢å•åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [
            index_order_to_vector_db.delay(order)
            for order in orders
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [task.get(timeout=int(os.getenv("CELERY_TASK_GET_TIMEOUT", "300"))) for task in tasks]

        success_count = sum(1 for r in results if r.get("success"))

        logger.info(
            "æ‰¹é‡ç´¢å¼•è®¢å•å®Œæˆ",
            total=len(orders),
            success=success_count,
            failed=len(orders) - success_count,
        )

        return {
            "success": True,
            "total": len(orders),
            "success_count": success_count,
            "failed_count": len(orders) - success_count,
        }

    except Exception as e:
        logger.error("æ‰¹é‡ç´¢å¼•è®¢å•å¤±è´¥", error=str(e), exc_info=e)
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
)
def batch_index_dishes(
    self,
    dishes: list[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    æ‰¹é‡ç´¢å¼•èœå“åˆ°å‘é‡æ•°æ®åº“

    Args:
        dishes: èœå“åˆ—è¡¨

    Returns:
        æ‰¹é‡ç´¢å¼•ç»“æœ
    """
    try:
        logger.info("å¼€å§‹æ‰¹é‡ç´¢å¼•èœå“", count=len(dishes))

        # ä¸ºæ¯ä¸ªèœå“åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = [
            index_dish_to_vector_db.delay(dish)
            for dish in dishes
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [task.get(timeout=int(os.getenv("CELERY_TASK_GET_TIMEOUT", "300"))) for task in tasks]

        success_count = sum(1 for r in results if r.get("success"))

        logger.info(
            "æ‰¹é‡ç´¢å¼•èœå“å®Œæˆ",
            total=len(dishes),
            success=success_count,
            failed=len(dishes) - success_count,
        )

        return {
            "success": True,
            "total": len(dishes),
            "success_count": success_count,
            "failed_count": len(dishes) - success_count,
        }

    except Exception as e:
        logger.error("æ‰¹é‡ç´¢å¼•èœå“å¤±è´¥", error=str(e), exc_info=e)
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),  # 5åˆ†é’Ÿ
)
def generate_and_send_daily_report(
    self,
    store_id: str = None,
    report_date: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¹¶å‘é€è¥ä¸šæ—¥æŠ¥

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºä¸ºæ‰€æœ‰é—¨åº—ç”Ÿæˆï¼ŒBeatè°ƒåº¦æ—¶ä½¿ç”¨)
        report_date: æŠ¥å‘Šæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©ï¼‰

    Returns:
        ç”Ÿæˆå’Œå‘é€ç»“æœ
    """
    async def _run():
        try:
            from datetime import date, datetime, timedelta
            from ..services.daily_report_service import daily_report_service
            from ..services.wechat_work_message_service import wechat_work_message_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..core.database import get_db_session
            from sqlalchemy import select

            # è§£ææ—¥æœŸ
            target_date = (
                datetime.strptime(report_date, "%Y-%m-%d").date()
                if report_date
                else date.today() - timedelta(days=1)
            )

            logger.info(
                "å¼€å§‹ç”Ÿæˆè¥ä¸šæ—¥æŠ¥",
                store_id=store_id,
                report_date=str(target_date)
            )

            # è·å–è¦ç”ŸæˆæŠ¥å‘Šçš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                stores = result.scalars().all()

            total_sent = 0
            for store in stores:
                try:
                    # 1. ç”Ÿæˆæ—¥æŠ¥
                    report = await daily_report_service.generate_daily_report(
                        store_id=str(store.id),
                        report_date=target_date
                    )

                    # 2. æ„å»ºæ¨é€æ¶ˆæ¯
                    message = f"""ã€è¥ä¸šæ—¥æŠ¥ã€‘{target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
é—¨åº—ï¼š{store.name}ï¼ˆ{store.id}ï¼‰

{report.summary}

ğŸ“Š è¯¦ç»†æ•°æ®ï¼š
â€¢ è®¢å•æ•°ï¼š{report.order_count}ç¬”
â€¢ å®¢æµé‡ï¼š{report.customer_count}äºº
â€¢ å®¢å•ä»·ï¼šÂ¥{report.avg_order_value / 100:.2f}

ğŸ“ˆ è¿è¥æŒ‡æ ‡ï¼š
â€¢ ä»»åŠ¡å®Œæˆç‡ï¼š{report.task_completion_rate:.1f}%
â€¢ åº“å­˜é¢„è­¦ï¼š{report.inventory_alert_count}ä¸ª
"""

                    if report.highlights:
                        message += "\nâœ¨ ä»Šæ—¥äº®ç‚¹ï¼š\n"
                        for highlight in report.highlights:
                            message += f"â€¢ {highlight}\n"

                    if report.alerts:
                        message += "\nâš ï¸ éœ€è¦å…³æ³¨ï¼š\n"
                        for alert in report.alerts:
                            message += f"â€¢ {alert}\n"

                    # 3. æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜ï¼Œå‘é€æ¨é€
                    async with get_db_session() as session:
                        mgr_result = await session.execute(
                            select(User).where(
                                User.store_id == store.id,
                                User.is_active == True,
                                User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                User.wechat_user_id.isnot(None)
                            )
                        )
                        managers = mgr_result.scalars().all()

                    sent_count = 0
                    for manager in managers:
                        try:
                            send_result = await wechat_work_message_service.send_text_message(
                                user_id=manager.wechat_user_id,
                                content=message
                            )
                            if send_result.get("success"):
                                sent_count += 1
                        except Exception as send_err:
                            logger.error(
                                "å‘é€æ—¥æŠ¥å¤±è´¥",
                                user_id=str(manager.id),
                                error=str(send_err)
                            )

                    # 4. æ ‡è®°ä¸ºå·²å‘é€
                    if sent_count > 0:
                        await daily_report_service.mark_as_sent(report.id)

                    logger.info(
                        "è¥ä¸šæ—¥æŠ¥ç”Ÿæˆå¹¶å‘é€å®Œæˆ",
                        store_id=str(store.id),
                        report_date=str(target_date),
                        sent_count=sent_count
                    )
                    total_sent += sent_count

                except Exception as store_err:
                    logger.error(
                        "é—¨åº—æ—¥æŠ¥ç”Ÿæˆå¤±è´¥",
                        store_id=str(store.id),
                        error=str(store_err)
                    )
                    continue

            logger.info(
                "æ‰€æœ‰é—¨åº—è¥ä¸šæ—¥æŠ¥ç”Ÿæˆå®Œæˆ",
                stores_processed=len(stores),
                total_sent=total_sent,
                report_date=str(target_date)
            )

            return {
                "success": True,
                "stores_processed": len(stores),
                "total_sent": total_sent,
                "report_date": str(target_date),
            }

        except Exception as e:
            logger.error(
                "ç”Ÿæˆè¥ä¸šæ—¥æŠ¥å¤±è´¥",
                store_id=store_id,
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),  # 5åˆ†é’Ÿ
)
def perform_daily_reconciliation(
    self,
    store_id: str,
    reconciliation_date: str = None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ¯æ—¥å¯¹è´¦

    Args:
        store_id: é—¨åº—ID
        reconciliation_date: å¯¹è´¦æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©ï¼‰

    Returns:
        å¯¹è´¦ç»“æœ
    """
    async def _run():
        try:
            from datetime import date, datetime
            from ..services.reconcile_service import reconcile_service

            logger.info(
                "å¼€å§‹æ‰§è¡Œæ¯æ—¥å¯¹è´¦",
                store_id=store_id,
                reconciliation_date=reconciliation_date
            )

            # è§£ææ—¥æœŸ
            if reconciliation_date:
                target_date = datetime.strptime(reconciliation_date, "%Y-%m-%d").date()
            else:
                from datetime import timedelta
                target_date = date.today() - timedelta(days=1)

            # æ‰§è¡Œå¯¹è´¦
            record = await reconcile_service.perform_reconciliation(
                store_id=store_id,
                reconciliation_date=target_date
            )

            logger.info(
                "æ¯æ—¥å¯¹è´¦å®Œæˆ",
                store_id=store_id,
                reconciliation_date=str(target_date),
                status=record.status.value,
                diff_ratio=record.diff_ratio
            )

            return {
                "success": True,
                "store_id": store_id,
                "reconciliation_date": str(target_date),
                "record_id": str(record.id),
                "status": record.status.value,
                "diff_ratio": record.diff_ratio,
                "alert_sent": record.alert_sent
            }

        except Exception as e:
            logger.error(
                "æ‰§è¡Œæ¯æ—¥å¯¹è´¦å¤±è´¥",
                store_id=store_id,
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def detect_revenue_anomaly(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    æ£€æµ‹è¥æ”¶å¼‚å¸¸ (æ¯15åˆ†é’Ÿæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºæ£€æµ‹æ‰€æœ‰é—¨åº—)

    Returns:
        æ£€æµ‹ç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime, timedelta, date
            from ..agents.decision_agent import DecisionAgent
            from ..services.wechat_alert_service import wechat_alert_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..models.order import Order, OrderStatus
            from ..core.database import get_db_session
            from sqlalchemy import select, func

            logger.info(
                "å¼€å§‹æ£€æµ‹è¥æ”¶å¼‚å¸¸",
                store_id=store_id
            )

            decision_agent = DecisionAgent()
            alerts_sent = 0

            # è·å–è¦æ£€æµ‹çš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        now = datetime.now()
                        today_start = datetime.combine(date.today(), datetime.min.time())

                        # å½“å‰è¥æ”¶ï¼šä»Šå¤©åˆ°ç›®å‰ä¸ºæ­¢å·²å®Œæˆ/å·²ä¸Šèœçš„è®¢å•
                        rev_result = await session.execute(
                            select(func.coalesce(func.sum(Order.final_amount), 0)).where(
                                Order.store_id == store.id,
                                Order.order_time >= today_start,
                                Order.order_time <= now,
                                Order.status.in_([OrderStatus.COMPLETED, OrderStatus.SERVED])
                            )
                        )
                        current_revenue = float(rev_result.scalar() or 0) / 100

                        # é¢„æœŸè¥æ”¶ï¼šè¿‡å»4å‘¨åŒæ˜ŸæœŸåŒæ—¶æ®µçš„å¹³å‡å€¼
                        current_elapsed = timedelta(hours=now.hour, minutes=now.minute)
                        expected_samples = []
                        for weeks_ago in range(1, 5):
                            past_date = date.today() - timedelta(weeks=weeks_ago)
                            past_start = datetime.combine(past_date, datetime.min.time())
                            past_end = past_start + current_elapsed
                            past_rev = await session.execute(
                                select(func.coalesce(func.sum(Order.final_amount), 0)).where(
                                    Order.store_id == store.id,
                                    Order.order_time >= past_start,
                                    Order.order_time <= past_end,
                                    Order.status.in_([OrderStatus.COMPLETED, OrderStatus.SERVED])
                                )
                            )
                            val = float(past_rev.scalar() or 0) / 100
                            if val > 0:
                                expected_samples.append(val)

                        if not expected_samples:
                            # æ— å†å²æ•°æ®ï¼Œè·³è¿‡æœ¬é—¨åº—
                            logger.debug("æ— å†å²è¥æ”¶æ•°æ®ï¼Œè·³è¿‡", store_id=str(store.id))
                            continue

                        expected_revenue = sum(expected_samples) / len(expected_samples)

                        # è®¡ç®—åå·®
                        deviation = ((current_revenue - expected_revenue) / expected_revenue) * 100

                        # åªæœ‰åå·®è¶…è¿‡é˜ˆå€¼æ‰å‘Šè­¦
                        if abs(deviation) > float(os.getenv("REVENUE_ANOMALY_THRESHOLD_PERCENT", "15")):
                            # ä½¿ç”¨DecisionAgentåˆ†æ
                            analysis = await decision_agent.analyze_revenue_anomaly(
                                store_id=str(store.id),
                                current_revenue=current_revenue,
                                expected_revenue=expected_revenue,
                                time_period="today"
                            )

                            if analysis["success"]:
                                # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®ID
                                user_result = await session.execute(
                                    select(User).where(
                                        User.store_id == store.id,
                                        User.is_active == True,
                                        User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                        User.wechat_user_id.isnot(None)
                                    )
                                )
                                managers = user_result.scalars().all()
                                recipient_ids = [m.wechat_user_id for m in managers]

                                if recipient_ids:
                                    # ä½¿ç”¨WeChatAlertServiceå‘é€å‘Šè­¦
                                    alert_result = await wechat_alert_service.send_revenue_alert(
                                        store_id=str(store.id),
                                        store_name=store.name,
                                        current_revenue=current_revenue,
                                        expected_revenue=expected_revenue,
                                        deviation=deviation,
                                        analysis=analysis['data']['analysis'],
                                        recipient_ids=recipient_ids
                                    )

                                    if alert_result.get("success"):
                                        alerts_sent += alert_result.get("sent_count", 0)
                                        logger.info(
                                            "è¥æ”¶å¼‚å¸¸å‘Šè­¦å·²å‘é€",
                                            store_id=str(store.id),
                                            deviation=deviation,
                                            sent_count=alert_result.get("sent_count")
                                        )
                                else:
                                    logger.warning(
                                        "æ— å¯ç”¨æ¥æ”¶äºº",
                                        store_id=str(store.id)
                                    )

                    except Exception as e:
                        logger.error(
                            "é—¨åº—è¥æ”¶å¼‚å¸¸æ£€æµ‹å¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "è¥æ”¶å¼‚å¸¸æ£€æµ‹å®Œæˆ",
                stores_checked=len(stores),
                alerts_sent=alerts_sent
            )

            return {
                "success": True,
                "stores_checked": len(stores),
                "alerts_sent": alerts_sent,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "è¥æ”¶å¼‚å¸¸æ£€æµ‹å¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),
)
def generate_daily_report_with_rag(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¹¶å‘é€æ˜¨æ—¥ç®€æŠ¥ (RAGå¢å¼ºç‰ˆï¼Œæ¯å¤©6AMæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºä¸ºæ‰€æœ‰é—¨åº—ç”Ÿæˆ)

    Returns:
        ç”Ÿæˆç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime, date, timedelta
            from ..agents.decision_agent import DecisionAgent
            from ..services.wechat_work_message_service import wechat_work_message_service
            from ..models.store import Store
            from ..core.database import get_db_session
            from sqlalchemy import select

            logger.info(
                "å¼€å§‹ç”Ÿæˆæ˜¨æ—¥ç®€æŠ¥(RAGå¢å¼º)",
                store_id=store_id
            )

            decision_agent = DecisionAgent()
            reports_sent = 0
            yesterday = date.today() - timedelta(days=1)

            # è·å–è¦ç”ŸæˆæŠ¥å‘Šçš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        # ä½¿ç”¨DecisionAgentç”Ÿæˆç»è¥å»ºè®®
                        recommendations = await decision_agent.generate_business_recommendations(
                            store_id=str(store.id),
                            focus_area=None  # å…¨é¢åˆ†æ
                        )

                        if recommendations["success"]:
                            # æ„å»ºç®€æŠ¥æ¶ˆæ¯
                            message = f"""ğŸ“Š æ˜¨æ—¥ç®€æŠ¥ {yesterday.strftime('%Yå¹´%mæœˆ%dæ—¥')}

    é—¨åº—: {store.name}

    AIç»è¥åˆ†æ:
    {recommendations['data']['recommendations']}

    ---
    åŸºäº{recommendations['data']['context_used']}æ¡å†å²æ•°æ®åˆ†æ
    ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    """

                            # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®IDå¹¶å‘é€
                            from ..models.user import User, UserRole
                            user_result = await session.execute(
                                select(User).where(
                                    User.store_id == store.id,
                                    User.is_active == True,
                                    User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                    User.wechat_user_id.isnot(None)
                                )
                            )
                            managers = user_result.scalars().all()
                            sent_count = 0
                            for manager in managers:
                                try:
                                    send_result = await wechat_work_message_service.send_text_message(
                                        user_id=manager.wechat_user_id,
                                        content=message
                                    )
                                    if send_result.get("success"):
                                        sent_count += 1
                                except Exception as send_err:
                                    logger.error(
                                        "å‘é€ç®€æŠ¥å¤±è´¥",
                                        user_id=str(manager.id),
                                        error=str(send_err)
                                    )

                            logger.info(
                                "æ˜¨æ—¥ç®€æŠ¥å·²ç”Ÿæˆå¹¶å‘é€",
                                store_id=str(store.id),
                                sent_count=sent_count
                            )
                            reports_sent += sent_count

                    except Exception as e:
                        logger.error(
                            "é—¨åº—ç®€æŠ¥ç”Ÿæˆå¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "æ˜¨æ—¥ç®€æŠ¥ç”Ÿæˆå®Œæˆ",
                stores_processed=len(stores),
                reports_sent=reports_sent
            )

            return {
                "success": True,
                "stores_processed": len(stores),
                "reports_sent": reports_sent,
                "report_date": str(yesterday),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "æ˜¨æ—¥ç®€æŠ¥ç”Ÿæˆå¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def check_inventory_alert(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    æ£€æŸ¥åº“å­˜é¢„è­¦ (åˆé«˜å³°å‰1å°æ—¶ï¼Œæ¯å¤©10AMæ‰§è¡Œ)

    Args:
        store_id: é—¨åº—ID (Noneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰é—¨åº—)

    Returns:
        æ£€æŸ¥ç»“æœ
    """
    async def _run():
        try:
            from datetime import datetime
            from ..agents.inventory_agent import InventoryAgent
            from ..services.wechat_alert_service import wechat_alert_service
            from ..models.store import Store
            from ..models.user import User, UserRole
            from ..models.inventory import InventoryItem, InventoryStatus
            from ..core.database import get_db_session
            from sqlalchemy import select

            logger.info(
                "å¼€å§‹æ£€æŸ¥åº“å­˜é¢„è­¦",
                store_id=store_id
            )

            inventory_agent = InventoryAgent()
            alerts_sent = 0

            # è·å–è¦æ£€æŸ¥çš„é—¨åº—åˆ—è¡¨
            async with get_db_session() as session:
                if store_id:
                    result = await session.execute(
                        select(Store).where(Store.id == store_id, Store.is_active == True)
                    )
                    stores = result.scalars().all()
                else:
                    result = await session.execute(
                        select(Store).where(Store.is_active == True)
                    )
                    stores = result.scalars().all()

                for store in stores:
                    try:
                        # ä»æ•°æ®åº“æŸ¥è¯¢ä½åº“å­˜/ç¼ºè´§åº“å­˜é¡¹
                        inv_result = await session.execute(
                            select(InventoryItem).where(
                                InventoryItem.store_id == store.id,
                                InventoryItem.status.in_([
                                    InventoryStatus.LOW,
                                    InventoryStatus.CRITICAL,
                                    InventoryStatus.OUT_OF_STOCK,
                                ])
                            )
                        )
                        low_stock_items = inv_result.scalars().all()

                        if not low_stock_items:
                            logger.debug("æ— åº“å­˜é¢„è­¦é¡¹", store_id=str(store.id))
                            continue

                        # æ„å»º InventoryAgent æ‰€éœ€çš„ current_inventory å­—å…¸
                        current_inventory = {
                            item.id: item.current_quantity for item in low_stock_items
                        }

                        # ä½¿ç”¨InventoryAgentæ£€æŸ¥ä½åº“å­˜
                        alert_result = await inventory_agent.check_low_stock_alert(
                            store_id=str(store.id),
                            current_inventory=current_inventory,
                            threshold_hours=int(os.getenv("INVENTORY_ALERT_THRESHOLD_HOURS", "4"))  # åˆé«˜å³°å‰Nå°æ—¶é¢„è­¦
                        )

                        if alert_result["success"]:
                            # æ„å»ºé¢„è­¦é¡¹ç›®åˆ—è¡¨ï¼ˆæ¥è‡ªçœŸå®æ•°æ®ï¼‰
                            alert_items = [
                                {
                                    "dish_name": item.name,
                                    "quantity": item.current_quantity,
                                    "unit": item.unit or "",
                                    "min_quantity": item.min_quantity,
                                    "risk": "high" if item.status in (
                                        InventoryStatus.CRITICAL, InventoryStatus.OUT_OF_STOCK
                                    ) else "medium",
                                }
                                for item in low_stock_items
                            ]

                            # æŸ¥è¯¢åº—é•¿å’Œç®¡ç†å‘˜çš„ä¼å¾®ID
                            user_result = await session.execute(
                                select(User).where(
                                    User.store_id == store.id,
                                    User.is_active == True,
                                    User.role.in_([UserRole.STORE_MANAGER, UserRole.ADMIN]),
                                    User.wechat_user_id.isnot(None)
                                )
                            )
                            managers = user_result.scalars().all()
                            recipient_ids = [m.wechat_user_id for m in managers]

                            if recipient_ids:
                                # ä½¿ç”¨WeChatAlertServiceå‘é€é¢„è­¦
                                send_result = await wechat_alert_service.send_inventory_alert(
                                    store_id=str(store.id),
                                    store_name=store.name,
                                    alert_items=alert_items,
                                    analysis=alert_result['data']['alert'],
                                    recipient_ids=recipient_ids
                                )

                                if send_result.get("success"):
                                    alerts_sent += send_result.get("sent_count", 0)
                                    logger.info(
                                        "åº“å­˜é¢„è­¦å·²å‘é€",
                                        store_id=str(store.id),
                                        sent_count=send_result.get("sent_count")
                                    )
                            else:
                                logger.warning(
                                    "æ— å¯ç”¨æ¥æ”¶äºº",
                                    store_id=str(store.id)
                                )

                    except Exception as e:
                        logger.error(
                            "é—¨åº—åº“å­˜æ£€æŸ¥å¤±è´¥",
                            store_id=str(store.id),
                            error=str(e)
                        )
                        continue

            logger.info(
                "åº“å­˜é¢„è­¦æ£€æŸ¥å®Œæˆ",
                stores_checked=len(stores),
                alerts_sent=alerts_sent
            )

            return {
                "success": True,
                "stores_checked": len(stores),
                "alerts_sent": alerts_sent,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(
                "åº“å­˜é¢„è­¦æ£€æŸ¥å¤±è´¥",
                error=str(e),
                exc_info=e
            )
            raise self.retry(exc=e)

    return asyncio.run(_run())


# ------------------------------------------------------------------ #
# å¤§æ•°æ®å¼‚æ­¥å¯¼å‡ºä»»åŠ¡                                                    #
# ------------------------------------------------------------------ #

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="async_export_data",
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def async_export_data(self, job_id: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤§æ•°æ®å¯¼å‡ºä»»åŠ¡

    ä»æ•°æ®åº“åˆ†æ‰¹è¯»å–æ•°æ®ï¼Œç”Ÿæˆ CSV/Excel æ–‡ä»¶ï¼Œ
    å¹¶å°†ç»“æœå†™å…¥ä¸´æ—¶ç›®å½•ï¼Œæ›´æ–° ExportJob çŠ¶æ€ã€‚
    """
    import csv
    import tempfile
    from datetime import datetime, date

    async def _run():
        from src.core.database import AsyncSessionLocal
        from src.models.export_job import ExportJob, ExportStatus
        from sqlalchemy import select, and_

        BATCH_SIZE = int(os.getenv("EXPORT_BATCH_SIZE", "1000"))

        async with AsyncSessionLocal() as session:
            job = await session.get(ExportJob, job_id)
            if not job:
                logger.error("å¯¼å‡ºä»»åŠ¡ä¸å­˜åœ¨", job_id=job_id)
                return {"success": False, "error": "job not found"}
            job.status = ExportStatus.RUNNING
            job.celery_task_id = self.request.id
            await session.commit()
            job_type = job.job_type
            fmt = job.format
            params = job.params or {}

        try:
            rows, headers = await _fetch_export_data(job_type, params)

            total = len(rows)
            tmp_dir = os.getenv("EXPORT_TMP_DIR", tempfile.gettempdir())
            os.makedirs(tmp_dir, exist_ok=True)
            filename = f"export_{job_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}.{fmt}"
            file_path = os.path.join(tmp_dir, filename)

            if fmt == "csv":
                file_size = _write_csv(file_path, headers, rows)
            elif fmt == "xlsx":
                file_size = _write_xlsx(file_path, headers, rows)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {fmt}")

            async with AsyncSessionLocal() as session:
                job = await session.get(ExportJob, job_id)
                if job:
                    job.status = ExportStatus.COMPLETED
                    job.progress = 100
                    job.total_rows = total
                    job.processed_rows = total
                    job.file_path = file_path
                    job.file_size_bytes = file_size
                    job.completed_at = datetime.utcnow().isoformat()
                    await session.commit()

            logger.info("å¯¼å‡ºä»»åŠ¡å®Œæˆ", job_id=job_id, total_rows=total)
            return {"success": True, "job_id": job_id, "total_rows": total}

        except Exception as e:
            logger.error("å¯¼å‡ºä»»åŠ¡å¤±è´¥", job_id=job_id, error=str(e))
            async with AsyncSessionLocal() as session:
                job = await session.get(ExportJob, job_id)
                if job:
                    job.status = ExportStatus.FAILED
                    job.error_message = str(e)
                    await session.commit()
            raise self.retry(exc=e)

    async def _fetch_export_data(job_type: str, params: Dict):
        from src.core.database import AsyncSessionLocal
        from sqlalchemy import select, and_
        from datetime import datetime, date

        if job_type == "transactions":
            from src.models.finance import FinancialTransaction
            headers = ["æ—¥æœŸ", "ç±»å‹", "åˆ†ç±»", "å­åˆ†ç±»", "é‡‘é¢(å…ƒ)", "æè¿°", "æ”¯ä»˜æ–¹å¼", "é—¨åº—ID"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("store_id"):
                    conditions.append(FinancialTransaction.store_id == params["store_id"])
                if params.get("transaction_type"):
                    conditions.append(FinancialTransaction.transaction_type == params["transaction_type"])
                if params.get("start_date"):
                    conditions.append(FinancialTransaction.transaction_date >= date.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(FinancialTransaction.transaction_date <= date.fromisoformat(params["end_date"]))
                stmt = select(FinancialTransaction)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(FinancialTransaction.transaction_date.desc())
                result = await session.execute(stmt)
                rows = [
                    [t.transaction_date.isoformat() if t.transaction_date else "",
                     t.transaction_type or "", t.category or "", t.subcategory or "",
                     round((t.amount or 0) / 100, 2), t.description or "",
                     t.payment_method or "", t.store_id or ""]
                    for t in result.scalars().all()
                ]
            return rows, headers

        elif job_type == "audit_logs":
            from src.models.audit_log import AuditLog
            headers = ["æ—¶é—´", "ç”¨æˆ·ID", "ç”¨æˆ·å", "è§’è‰²", "æ“ä½œ", "èµ„æºç±»å‹", "èµ„æºID", "æè¿°", "IP", "çŠ¶æ€", "é—¨åº—ID"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("user_id"):
                    conditions.append(AuditLog.user_id == params["user_id"])
                if params.get("action"):
                    conditions.append(AuditLog.action == params["action"])
                if params.get("store_id"):
                    conditions.append(AuditLog.store_id == params["store_id"])
                if params.get("start_date"):
                    conditions.append(AuditLog.created_at >= datetime.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(AuditLog.created_at <= datetime.fromisoformat(params["end_date"]))
                stmt = select(AuditLog)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(AuditLog.created_at.desc())
                result = await session.execute(stmt)
                rows = [
                    [log.created_at.isoformat() if log.created_at else "",
                     str(log.user_id) if log.user_id else "", log.username or "",
                     log.user_role or "", log.action or "", log.resource_type or "",
                     str(log.resource_id) if log.resource_id else "", log.description or "",
                     log.ip_address or "", log.status or "",
                     str(log.store_id) if log.store_id else ""]
                    for log in result.scalars().all()
                ]
            return rows, headers

        elif job_type == "orders":
            from src.models.order import Order
            headers = ["è®¢å•å·", "çŠ¶æ€", "æ€»é‡‘é¢(å…ƒ)", "æ¡Œå·", "é—¨åº—ID", "ä¸‹å•æ—¶é—´"]
            async with AsyncSessionLocal() as session:
                conditions = []
                if params.get("store_id"):
                    conditions.append(Order.store_id == params["store_id"])
                if params.get("status"):
                    conditions.append(Order.status == params["status"])
                if params.get("start_date"):
                    conditions.append(Order.created_at >= datetime.fromisoformat(params["start_date"]))
                if params.get("end_date"):
                    conditions.append(Order.created_at <= datetime.fromisoformat(params["end_date"]))
                stmt = select(Order)
                if conditions:
                    stmt = stmt.where(and_(*conditions))
                stmt = stmt.order_by(Order.created_at.desc())
                result = await session.execute(stmt)
                rows = [
                    [o.order_number or str(o.id),
                     o.status.value if hasattr(o.status, "value") else str(o.status or ""),
                     round((o.total_amount or 0) / 100, 2),
                     o.table_number or "", o.store_id or "",
                     o.created_at.isoformat() if o.created_at else ""]
                    for o in result.scalars().all()
                ]
            return rows, headers

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {job_type}ï¼Œå¯é€‰: transactions/audit_logs/orders")

    def _write_csv(file_path: str, headers: list, rows: list) -> int:
        with open(file_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(rows)
        return os.path.getsize(file_path)

    def _write_xlsx(file_path: str, headers: list, rows: list) -> int:
        try:
            import openpyxl
            from openpyxl.styles import Font, PatternFill
            from openpyxl.utils import get_column_letter
        except ImportError:
            raise ImportError("è¯·å®‰è£… openpyxl: pip install openpyxl")
        wb = openpyxl.Workbook()
        ws = wb.active
        hf = Font(bold=True, color="FFFFFF")
        hfill = PatternFill(start_color="2F5496", end_color="2F5496", fill_type="solid")
        for ci, h in enumerate(headers, 1):
            c = ws.cell(row=1, column=ci, value=h)
            c.font = hf
            c.fill = hfill
            ws.column_dimensions[get_column_letter(ci)].width = 16
        for ri, row in enumerate(rows, 2):
            for ci, v in enumerate(row, 1):
                ws.cell(row=ri, column=ci, value=v)
        wb.save(file_path)
        return os.path.getsize(file_path)

    return asyncio.run(_run())


# ---------------------------------------------------------------------------
# å¢é‡å¤‡ä»½ä»»åŠ¡
# ---------------------------------------------------------------------------

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="run_backup",
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def run_backup(self, job_id: str) -> Dict[str, Any]:
    """
    æ‰§è¡Œå…¨é‡/å¢é‡å¤‡ä»½ä»»åŠ¡
    - å…¨é‡ï¼šå¯¼å‡ºæ‰€æœ‰æŒ‡å®šè¡¨çš„æ•°æ®ä¸º JSONï¼Œæ‰“åŒ…æˆ tar.gz
    - å¢é‡ï¼šä»…å¯¼å‡º since_timestamp ä¹‹åæœ‰å˜æ›´çš„è¡Œï¼ˆä¾èµ– updated_at å­—æ®µï¼‰
    """
    import hashlib
    import json
    import tarfile
    import tempfile
    from datetime import datetime, timezone

    async def _run():
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from sqlalchemy import text

        db_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://postgres:postgres@localhost/zhilian")
        backup_dir = os.getenv("BACKUP_TMP_DIR", "/tmp/backups")
        os.makedirs(backup_dir, exist_ok=True)

        engine = create_async_engine(db_url, echo=False)
        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

        async with async_session() as session:
            # è¯»å– BackupJob
            from src.models.backup_job import BackupJob, BackupStatus
            result = await session.execute(
                text("SELECT * FROM backup_jobs WHERE id = :id"),
                {"id": job_id},
            )
            row = result.mappings().first()
            if not row:
                raise ValueError(f"BackupJob {job_id} ä¸å­˜åœ¨")

            backup_type = row["backup_type"]
            since_ts = row["since_timestamp"]
            tables_filter = row["tables"] or []

            # æ ‡è®° RUNNING
            await session.execute(
                text("UPDATE backup_jobs SET status='running', celery_task_id=:tid, updated_at=NOW() WHERE id=:id"),
                {"tid": self.request.id, "id": job_id},
            )
            await session.commit()

        # è·å–æ‰€æœ‰ç”¨æˆ·è¡¨
        async with async_session() as session:
            res = await session.execute(
                text("SELECT tablename FROM pg_tables WHERE schemaname='public' ORDER BY tablename")
            )
            all_tables = [r[0] for r in res.fetchall()]

        target_tables = [t for t in all_tables if not tables_filter or t in tables_filter]
        # æ’é™¤å¤‡ä»½ç›¸å…³è¡¨ï¼Œé¿å…é€’å½’
        target_tables = [t for t in target_tables if t not in ("backup_jobs", "export_jobs")]

        total = len(target_tables)
        row_counts: Dict[str, int] = {}
        tmp_dir = tempfile.mkdtemp(dir=backup_dir)

        try:
            for idx, table in enumerate(target_tables):
                async with async_session() as session:
                    if backup_type == "incremental" and since_ts:
                        # å¢é‡ï¼šåªå– updated_at > since_timestamp çš„è¡Œ
                        try:
                            res = await session.execute(
                                text(f"SELECT * FROM {table} WHERE updated_at > :ts"),
                                {"ts": since_ts},
                            )
                        except Exception:
                            # è¡¨æ²¡æœ‰ updated_at å­—æ®µæ—¶è·³è¿‡
                            row_counts[table] = 0
                            continue
                    else:
                        res = await session.execute(text(f"SELECT * FROM {table}"))

                    cols = list(res.keys())
                    rows_data = [dict(zip(cols, r)) for r in res.fetchall()]

                    # åºåˆ—åŒ–ï¼ˆUUID/datetime è½¬å­—ç¬¦ä¸²ï¼‰
                    def _serialize(v):
                        if hasattr(v, "isoformat"):
                            return v.isoformat()
                        if hasattr(v, "__str__") and not isinstance(v, (int, float, bool, str, type(None))):
                            return str(v)
                        return v

                    rows_data = [{k: _serialize(v) for k, v in r.items()} for r in rows_data]
                    row_counts[table] = len(rows_data)

                    table_file = os.path.join(tmp_dir, f"{table}.json")
                    with open(table_file, "w", encoding="utf-8") as f:
                        json.dump({"table": table, "rows": rows_data}, f, ensure_ascii=False, indent=2)

                # æ›´æ–°è¿›åº¦
                progress = int((idx + 1) / total * 90)
                async with async_session() as session:
                    await session.execute(
                        text("UPDATE backup_jobs SET progress=:p, updated_at=NOW() WHERE id=:id"),
                        {"p": progress, "id": job_id},
                    )
                    await session.commit()

            # æ‰“åŒ… tar.gz
            ts_str = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
            archive_name = f"backup_{backup_type}_{ts_str}_{job_id[:8]}.tar.gz"
            archive_path = os.path.join(backup_dir, archive_name)
            with tarfile.open(archive_path, "w:gz") as tar:
                tar.add(tmp_dir, arcname="backup")

            # è®¡ç®— SHA256
            sha256 = hashlib.sha256()
            with open(archive_path, "rb") as f:
                for chunk in iter(lambda: f.read(65536), b""):
                    sha256.update(chunk)
            checksum = sha256.hexdigest()
            file_size = os.path.getsize(archive_path)
            completed_at = datetime.now(timezone.utc).isoformat()

            async with async_session() as session:
                await session.execute(
                    text(
                        "UPDATE backup_jobs SET status='completed', progress=100, "
                        "file_path=:fp, file_size_bytes=:fs, checksum=:cs, "
                        "row_counts=:rc, completed_at=:ca, updated_at=NOW() WHERE id=:id"
                    ),
                    {
                        "fp": archive_path,
                        "fs": file_size,
                        "cs": checksum,
                        "rc": json.dumps(row_counts),
                        "ca": completed_at,
                        "id": job_id,
                    },
                )
                await session.commit()

            logger.info("å¤‡ä»½ä»»åŠ¡å®Œæˆ", job_id=job_id, archive=archive_path, checksum=checksum)
            return {"job_id": job_id, "file_path": archive_path, "checksum": checksum}

        except Exception as e:
            logger.error("å¤‡ä»½ä»»åŠ¡å¤±è´¥", job_id=job_id, error=str(e))
            async with async_session() as session:
                await session.execute(
                    text("UPDATE backup_jobs SET status='failed', error_message=:err, updated_at=NOW() WHERE id=:id"),
                    {"err": str(e)[:1000], "id": job_id},
                )
                await session.commit()
            raise self.retry(exc=e)

        finally:
            import shutil
            shutil.rmtree(tmp_dir, ignore_errors=True)

    return asyncio.run(_run())


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),
)
def generate_daily_hub(
    self,
    store_id: str = None,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆ T+1 ç»è¥ç»Ÿç­¹å¤‡æˆ˜æ¿

    Args:
        store_id: é—¨åº—ID (None è¡¨ç¤ºä¸ºæ‰€æœ‰æ´»è·ƒé—¨åº—ç”Ÿæˆ)

    Returns:
        ç”Ÿæˆç»“æœ
    """
    async def _run():
        from datetime import date, timedelta
        from ..services.daily_hub_service import daily_hub_service
        from ..models.store import Store
        from ..core.database import get_db_session
        from sqlalchemy import select

        target_date = date.today() + timedelta(days=1)

        async with get_db_session() as session:
            if store_id:
                result = await session.execute(
                    select(Store).where(Store.id == store_id, Store.is_active == True)
                )
            else:
                result = await session.execute(
                    select(Store).where(Store.is_active == True)
                )
            stores = result.scalars().all()

        generated = 0
        for store in stores:
            try:
                await daily_hub_service.generate_battle_board(
                    store_id=str(store.id), target_date=target_date
                )
                generated += 1
                logger.info("å¤‡æˆ˜æ¿ç”ŸæˆæˆåŠŸ", store_id=str(store.id), target_date=str(target_date))
            except Exception as e:
                logger.error("å¤‡æˆ˜æ¿ç”Ÿæˆå¤±è´¥", store_id=str(store.id), error=str(e))

        return {"success": True, "generated": generated, "target_date": str(target_date)}

    try:
        return asyncio.run(_run())
    except Exception as e:
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_SHORT", "30")),
)
def dispatch_training_recommendation(
    self,
    store_id: str,
    tenant_id: str,
    root_cause_dimension: str,
    affected_staff_ids: list,
    waste_event_id: str,
) -> Dict[str, Any]:
    """
    åºŸæ–™æ ¹å›  â†’ åŸ¹è®­æ¨èåˆ†å‘ã€‚

    æ ¹æ®æŸè€—æ¨ç†æ ¹å› ç»´åº¦æŸ¥è¯¢ ROOT_CAUSE_TO_TRAINING é…ç½®ï¼Œ
    ä¸ºå½“ç­å‘˜å·¥æ‰¹é‡åˆ›å»ºé’ˆå¯¹æ€§åŸ¹è®­æ¨èè®°å½•ï¼Œ
    å¹¶é€šè¿‡ AgentMemoryBus é€šçŸ¥ TrainingAgentã€‚

    ç”± run_waste_reasoning() åœ¨ top3 æ ¹å› ç¡®å®šåè§¦å‘ã€‚
    """
    async def _run():
        from src.core.root_cause_config import ROOT_CAUSE_TO_TRAINING
        from src.services.training_service import TrainingService
        from src.services.agent_memory_bus import agent_memory_bus

        config = ROOT_CAUSE_TO_TRAINING.get(root_cause_dimension)
        if not config:
            logger.info(
                "waste_training_dispatch_no_config",
                root_cause=root_cause_dimension,
                store_id=store_id,
            )
            return {"skipped": True, "reason": "no_mapping_config", "root_cause": root_cause_dimension}

        training_svc = TrainingService(store_id=store_id)
        created = []
        for staff_id in affected_staff_ids:
            try:
                rec = await training_svc.create_waste_driven_recommendation(
                    staff_id=staff_id,
                    root_cause=root_cause_dimension,
                    waste_event_id=waste_event_id,
                    course_ids=config["course_ids"],
                    urgency=config["urgency"],
                    urgency_days=config.get("urgency_days", 7),
                    skill_gap=config["skill_gap"],
                    description=config["description"],
                )
                created.append(rec)
            except Exception as staff_err:
                logger.warning(
                    "waste_training_dispatch_staff_failed",
                    staff_id=staff_id,
                    error=str(staff_err),
                )

        # é€šçŸ¥ AgentMemoryBusï¼ŒTrainingAgent å¯è®¢é˜…æ­¤äº‹ä»¶
        await agent_memory_bus.publish(
            store_id=store_id,
            agent_id="waste_reasoning",
            action="training_recommendation_dispatched",
            summary=(
                f"æ ¹å› [{root_cause_dimension}]è§¦å‘åŸ¹è®­æ¨èï¼š{config['skill_gap']}ï¼Œ"
                f"å…±{len(created)}ä½å‘˜å·¥ï¼Œç´§è¿«åº¦{config['urgency']}"
            ),
            confidence=0.85,
            data={
                "root_cause": root_cause_dimension,
                "waste_event_id": waste_event_id,
                "affected_staff_count": len(created),
                "course_ids": config["course_ids"],
                "urgency": config["urgency"],
                "skill_gap": config["skill_gap"],
            },
        )

        # Phase 1.3: å†™å…¥ Neo4j Staff-Training å…³ç³»ï¼Œå…³é—­å› æœå›¾é—­ç¯
        from datetime import datetime as _dt_neo
        from src.ontology import get_ontology_repository
        repo = get_ontology_repository()
        if repo and created:
            # ä½¿ç”¨æ ¹å› ç»´åº¦ + æŠ€èƒ½ç¼ºå£ä½œä¸º TrainingModule å”¯ä¸€ ID
            module_id = f"tm_{root_cause_dimension}_{config['skill_gap'].replace(' ', '_')}"
            try:
                repo.merge_training_module(
                    module_id=module_id,
                    name=config["description"],
                    skill_gap=config["skill_gap"],
                    course_ids=config["course_ids"],
                    tenant_id=tenant_id,
                )
                for rec in created:
                    s_id = rec.get("staff_id", "")
                    if s_id:
                        repo.staff_needs_training(
                            staff_id=s_id,
                            module_id=module_id,
                            waste_event_id=waste_event_id,
                            urgency=config["urgency"],
                            deadline=rec.get("deadline", ""),
                        )
            except Exception as neo_err:
                logger.warning(
                    "neo4j_staff_needs_training_failed",
                    store_id=store_id,
                    error=str(neo_err),
                )

        logger.info(
            "waste_training_dispatch_done",
            store_id=store_id,
            root_cause=root_cause_dimension,
            waste_event_id=waste_event_id,
            staff_count=len(created),
        )

        # ä¼å¾®å®æ—¶å‘Šè­¦ï¼šé€šçŸ¥é—¨åº—ç®¡ç†å‘˜æŸè€—æ ¹å› ä¸åŸ¹è®­æ¨è
        try:
            from src.services.wechat_alert_service import wechat_alert_service as _wechat_svc
            await _wechat_svc.send_waste_training_alert(
                store_id=store_id,
                root_cause=root_cause_dimension,
                skill_gap=config["skill_gap"],
                urgency=config["urgency"],
                affected_staff_count=len(created),
                course_ids=config["course_ids"],
                waste_event_id=waste_event_id,
            )
        except Exception as alert_err:
            logger.warning(
                "waste_training_wechat_alert_failed",
                store_id=store_id,
                root_cause=root_cause_dimension,
                error=str(alert_err),
            )

        # Phase 2.1: 7å¤©åè§¦å‘åŸ¹è®­æ•ˆæœéªŒè¯
        from datetime import datetime as _dt, timedelta
        eta_7d = _dt.utcnow() + timedelta(days=7)
        for rec in created:
            try:
                verify_training_effectiveness.apply_async(
                    kwargs={
                        "store_id": store_id,
                        "staff_id": rec.get("staff_id", ""),
                        "waste_event_id": waste_event_id,
                        "root_cause": root_cause_dimension,
                    },
                    eta=eta_7d,
                )
            except Exception as sched_err:
                logger.warning(
                    "verify_training_schedule_failed",
                    staff_id=rec.get("staff_id"),
                    error=str(sched_err),
                )

        return {
            "store_id": store_id,
            "root_cause": root_cause_dimension,
            "waste_event_id": waste_event_id,
            "created": len(created),
            "recommendations": created,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("dispatch_training_recommendation_failed", store_id=store_id, error=str(e))
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def escalate_ontology_actions(self) -> Dict[str, Any]:
    """
    L4 Action è¶…æ—¶è‡ªåŠ¨å‡çº§ï¼šæ‰«æå·² SENT ä¸”è¶…è¿‡ deadline æœªå›æ‰§çš„ Actionï¼Œ
    æ ‡è®° escalation_at / escalated_to å¹¶æ¨é€ç»™é…ç½®çš„å‡çº§å¯¹è±¡ï¼ˆä¼å¾®ï¼‰ã€‚
    ç”± Celery Beat æ¯ 5â€“10 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ã€‚
    """
    async def _run():
        from src.core.database import get_db_session
        from src.services.ontology_action_service import process_escalations

        async with get_db_session() as session:
            n = await process_escalations(session)
            await session.commit()
        return {"escalated": n}

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("escalate_ontology_actions_failed", error=str(e))
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY", "60")),
)
def verify_training_effectiveness(
    self,
    store_id: str,
    staff_id: str,
    waste_event_id: str,
    root_cause: str,
    pre_training_period_days: int = 7,
) -> Dict[str, Any]:
    """
    Phase 2.1 åŸ¹è®­æ•ˆæœéªŒè¯ï¼š

    åœ¨åŸ¹è®­æ¨èåˆ›å»º 7 å¤©åï¼ˆETA å»¶è¿Ÿè§¦å‘ï¼‰ï¼Œå¯¹æ¯”åŸ¹è®­å‰ååŒå‘˜å·¥/åŒæ ¹å› çš„åºŸæ–™ç‡ã€‚
    ç»“æœå†™å…¥ agent_memory_busï¼Œä¾› TrainingAgent å’Œ KnowledgeRuleService ä½¿ç”¨ã€‚
    """
    async def _run():
        from datetime import datetime, timedelta
        from sqlalchemy import select, func, and_
        from src.core.database import get_db_session
        from src.models.kpi import KPI, KPIRecord
        from src.services.agent_memory_bus import agent_memory_bus

        now = datetime.now()
        pre_start = now - timedelta(days=pre_training_period_days * 2)
        pre_end = now - timedelta(days=pre_training_period_days)
        post_start = pre_end
        post_end = now

        async with get_db_session() as session:
            # æŸ¥è¯¢åŸ¹è®­å‰ååŒå‘˜å·¥çš„ waste_driven_training è®°å½•
            def _query_waste_kpi(from_dt, to_dt):
                return (
                    select(func.avg(KPIRecord.value).label("avg_score"))
                    .join(KPI, KPIRecord.kpi_id == KPI.id)
                    .where(
                        and_(
                            KPIRecord.store_id == store_id,
                            KPI.category == "waste_driven_training",
                            KPIRecord.record_date >= from_dt.date(),
                            KPIRecord.record_date <= to_dt.date(),
                        )
                    )
                )

            pre_result = await session.execute(_query_waste_kpi(pre_start, pre_end))
            post_result = await session.execute(_query_waste_kpi(post_start, post_end))

            pre_score = pre_result.scalar() or 0
            post_score = post_result.scalar() or 0

        improvement = post_score - pre_score
        effectiveness = min(100.0, max(0.0, 50.0 + improvement))

        # å†™å…¥ agent_memory_bus
        await agent_memory_bus.publish(
            store_id=store_id,
            agent_id="training_verifier",
            action="training_effectiveness_verified",
            summary=(
                f"å‘˜å·¥[{staff_id}] æ ¹å› [{root_cause}] åŸ¹è®­æ•ˆæœï¼š"
                f"è®­å‰å¾—åˆ†{pre_score:.1f} â†’ è®­å{post_score:.1f}ï¼Œ"
                f"æ”¹å–„{improvement:+.1f}ï¼Œæœ‰æ•ˆæ€§{effectiveness:.0f}%"
            ),
            confidence=0.75,
            data={
                "staff_id": staff_id,
                "waste_event_id": waste_event_id,
                "root_cause": root_cause,
                "pre_score": pre_score,
                "post_score": post_score,
                "improvement": improvement,
                "effectiveness": effectiveness,
            },
        )

        logger.info(
            "training_effectiveness_verified",
            store_id=store_id,
            staff_id=staff_id,
            root_cause=root_cause,
            effectiveness=effectiveness,
        )

        # Phase 2.2: æ›´æ–°çŸ¥è¯†åº“ä¸­å¯¹åº”æ ¹å› çš„ waste_rule ç²¾åº¦ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
        try:
            from src.services.ontology_knowledge_service import update_knowledge_accuracy
            # tenant_id ä» store_id åæŸ¥ï¼Œæˆ–ç›´æ¥ç”¨ store_id æ‰€å± tenantï¼ˆæ­¤å¤„ç®€åŒ–ä¸ºæŒ‰ store_id è¿‡æ»¤æ‰€æœ‰ç§Ÿæˆ·ï¼‰
            update_knowledge_accuracy(
                root_cause=root_cause,
                effectiveness=effectiveness,
                tenant_id="",  # ç©ºå­—ç¬¦ä¸²è·³è¿‡ tenant è¿‡æ»¤ï¼Œå…¨å±€åŒ¹é…
            )
        except Exception as ka_err:
            logger.warning("knowledge_accuracy_update_failed", root_cause=root_cause, error=str(ka_err))

        return {
            "store_id": store_id,
            "staff_id": staff_id,
            "root_cause": root_cause,
            "pre_score": pre_score,
            "post_score": post_score,
            "improvement": improvement,
            "effectiveness": effectiveness,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("verify_training_effectiveness_failed", store_id=store_id, error=str(e))
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    max_retries=int(os.getenv("CELERY_MAX_RETRIES", "3")),
    default_retry_delay=int(os.getenv("CELERY_RETRY_DELAY_LONG", "300")),
)
def propagate_training_knowledge(self) -> Dict[str, Any]:
    """
    Phase 3.2 è·¨é—¨åº—åŸ¹è®­çŸ¥è¯†ä¼ æ’­ï¼ˆå‘¨é¢‘å®šæ—¶ä»»åŠ¡ï¼‰ï¼š

    æŸ¥è¯¢å„é—¨åº—åºŸæ–™ç‡æ”¹å–„ Top3 çš„åŸ¹è®­æ–¹æ¡ˆï¼Œå‘ç›¸ä¼¼é—¨åº—è‡ªåŠ¨åˆ›å»ºåŸ¹è®­å»ºè®®ï¼Œ
    æ ‡è®°æ¥æºä¸º cross_store_best_practiceã€‚
    ç”± Celery Beat æ¯å‘¨ä¸€æ¬¡æ‰§è¡Œã€‚
    """
    async def _run():
        from sqlalchemy import select, func, and_
        from src.core.database import get_db_session
        from src.models.kpi import KPI, KPIRecord
        from src.models.store import Store
        from src.services.training_service import TrainingService

        propagated = 0
        async with get_db_session() as session:
            # æŸ¥è¯¢æ‰€æœ‰é—¨åº—
            stores_result = await session.execute(select(Store))
            stores = stores_result.scalars().all()
            store_ids = [str(s.id) for s in stores]

            # æŸ¥è¯¢å„é—¨åº—åŸ¹è®­è®°å½•ï¼Œæ‰¾åºŸæ–™ç‡æ”¹å–„æœ€å¥½çš„ï¼ˆå¾—åˆ†æœ€é«˜ï¼‰
            top_practices = {}
            for sid in store_ids:
                stmt = (
                    select(
                        KPIRecord.kpi_id,
                        func.avg(KPIRecord.value).label("avg_score"),
                        func.count(KPIRecord.id).label("cnt"),
                    )
                    .join(KPI, KPIRecord.kpi_id == KPI.id)
                    .where(
                        and_(
                            KPIRecord.store_id == sid,
                            KPI.category == "waste_driven_training",
                            KPIRecord.status == "on_track",
                        )
                    )
                    .group_by(KPIRecord.kpi_id)
                    .order_by(func.avg(KPIRecord.value).desc())
                    .limit(3)
                )
                result = await session.execute(stmt)
                rows = result.all()
                if rows:
                    top_practices[sid] = [
                        {"kpi_id": r.kpi_id, "avg_score": float(r.avg_score), "cnt": r.cnt}
                        for r in rows
                    ]

            # å‘ç›¸ä¼¼é—¨åº—ä¼ æ’­æœ€ä½³å®è·µï¼›è‹¥ Neo4j æœªé…ç½®åˆ™é™çº§ä¸ºå…¨é—¨åº—å¹¿æ’­
            from src.ontology import get_ontology_repository
            neo_repo = get_ontology_repository()

            for source_sid, practices in top_practices.items():
                # Phase 3: ä¼˜å…ˆé€šè¿‡ SIMILAR_TO å…³ç³»ç¼©å°ä¼ æ’­èŒƒå›´
                if neo_repo:
                    try:
                        similar = neo_repo.get_similar_stores(source_sid, min_score=0.5)
                        target_sids = [s["store_id"] for s in similar] if similar else [
                            sid for sid in store_ids if sid != source_sid
                        ]
                    except Exception:
                        target_sids = [sid for sid in store_ids if sid != source_sid]
                else:
                    target_sids = [sid for sid in store_ids if sid != source_sid]

                for target_sid in target_sids:
                    for practice in practices[:1]:  # æ¯å®¶é—¨åº—åªä¼ æ’­ Top1
                        try:
                            svc = TrainingService(store_id=target_sid)
                            kpi_id = practice["kpi_id"]
                            # ä» kpi_id è§£æ root_cause (æ ¼å¼: KPI_WASTE_{ROOT_CAUSE}_...)
                            parts = kpi_id.split("_")
                            root_cause = parts[2].lower() if len(parts) > 2 else "cross_store"
                            await svc.create_waste_driven_recommendation(
                                staff_id="STORE_GENERAL",
                                root_cause=f"cross_store_{root_cause}",
                                waste_event_id=f"cross_store_{source_sid}_{root_cause}",
                                course_ids=[kpi_id],
                                urgency="low",
                                urgency_days=30,
                                skill_gap=root_cause,
                                description=(
                                    f"è·¨é—¨åº—æœ€ä½³å®è·µï¼šæ¥è‡ªé—¨åº—[{source_sid}]ï¼Œ"
                                    f"åŸ¹è®­[{kpi_id}]å¹³å‡å¾—åˆ†{practice['avg_score']:.1f}"
                                ),
                            )
                            propagated += 1
                        except Exception as e:
                            logger.warning(
                                "cross_store_propagate_failed",
                                source=source_sid,
                                target=target_sid,
                                error=str(e),
                            )

        logger.info("cross_store_training_propagated", count=propagated)
        return {"propagated": propagated}

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("propagate_training_knowledge_failed", error=str(e))
        raise self.retry(exc=e)


# ============================================================
# å›¾è°±å®šæœŸåŒæ­¥ï¼ˆæ¯æ—¥å‡Œæ™¨ 2AMï¼ŒPG â†’ Neo4jï¼‰
# ============================================================

@celery_app.task(
    bind=True,
    name="src.core.celery_tasks.sync_ontology_graph",
    max_retries=2,
    default_retry_delay=300,
)
def sync_ontology_graph(self, tenant_id: str = "") -> Dict[str, Any]:
    """
    æ¯æ—¥å®šæ—¶å°† PostgreSQL ä¸»æ•°æ®åŒæ­¥åˆ° Neo4j å›¾è°±ï¼ˆL2 æœ¬ä½“å±‚ï¼‰ã€‚

    åŒæ­¥å†…å®¹ï¼šStoreï¼ˆå«ç›¸ä¼¼åº¦è‡ªåŠ¨è®¡ç®—ï¼‰ã€Dishã€Ingredientã€Staffã€Orderã€‚
    tenant_id ä¸ºç©ºæ—¶ä½¿ç”¨ç¯å¢ƒå˜é‡ DEFAULT_TENANT_IDï¼Œä»ä¸ºç©ºåˆ™ä½¿ç”¨ "default"ã€‚
    ç”± Celery Beat æ¯æ—¥å‡Œæ™¨ 2AM è§¦å‘ï¼›ä¹Ÿå¯æ‰‹åŠ¨è°ƒç”¨ POST /ontology/sync-from-pgã€‚
    """
    import os as _os

    async def _run():
        from src.core.database import get_db_session
        from src.services.ontology_sync_service import sync_ontology_from_pg

        effective_tenant = tenant_id or _os.getenv("DEFAULT_TENANT_ID", "default")

        async with get_db_session() as session:
            result = await sync_ontology_from_pg(session, tenant_id=effective_tenant)

        logger.info(
            "ontology_graph_synced",
            tenant_id=effective_tenant,
            stores=result.get("stores", 0),
            staff=result.get("staff", 0),
            dishes=result.get("dishes", 0),
            ingredients=result.get("ingredients", 0),
            orders=result.get("orders", 0),
        )
        return {"ok": True, "tenant_id": effective_tenant, **result}

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("sync_ontology_graph_failed", error=str(e))
        raise self.retry(exc=e)


# ============================================================
# ARCH-003: é—¨åº—è®°å¿†å±‚ Celery ä»»åŠ¡
# ============================================================

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="src.core.celery_tasks.update_store_memory",
    max_retries=2,
    default_retry_delay=300,
)
def update_store_memory(self, store_id: str = None, brand_id: str = None) -> Dict[str, Any]:
    """
    æ¯æ—¥å‡Œæ™¨2ç‚¹æ›´æ–°é—¨åº—è®°å¿†å±‚ï¼ˆCelery Beat è°ƒåº¦ï¼‰

    Args:
        store_id: æŒ‡å®šé—¨åº—IDï¼ˆNone è¡¨ç¤ºæ›´æ–°æ‰€æœ‰æ´»è·ƒé—¨åº—ï¼‰
        brand_id: å“ç‰ŒIDï¼ˆå¯é€‰ï¼‰

    Returns:
        æ›´æ–°ç»“æœ
    """
    async def _run():
        from ..models.store import Store
        from ..core.database import get_db_session
        from ..services.store_memory_service import StoreMemoryService
        from sqlalchemy import select

        service = StoreMemoryService()
        updated = 0
        failed = 0

        async with get_db_session() as session:
            if store_id:
                result = await session.execute(
                    select(Store).where(Store.id == store_id, Store.is_active == True)
                )
                stores = result.scalars().all()
            else:
                result = await session.execute(
                    select(Store).where(Store.is_active == True)
                )
                stores = result.scalars().all()

        for store in stores:
            try:
                memory = await service.refresh_store_memory(
                    store_id=str(store.id),
                    brand_id=getattr(store, 'brand_id', None) or brand_id,
                    lookback_days=30,
                )
                updated += 1
                logger.info("store_memory.updated", store_id=str(store.id), confidence=memory.confidence)
            except Exception as e:
                failed += 1
                logger.error("store_memory.update_failed", store_id=str(store.id), error=str(e))

        logger.info("update_store_memory.done", updated=updated, failed=failed)
        return {"updated": updated, "failed": failed}

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("update_store_memory_failed", error=str(e))
        raise self.retry(exc=e)


@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="src.core.celery_tasks.realtime_anomaly_check",
    max_retries=1,
    default_retry_delay=10,
)
def realtime_anomaly_check(self, store_id: str, event: Dict[str, Any]) -> Dict[str, Any]:
    """
    å®æ—¶å¼‚å¸¸æ£€æµ‹ï¼ˆStaffAction å†™å…¥åè§¦å‘ï¼‰

    æ£€æµ‹è¯¥é—¨åº—æœ€æ–°æ“ä½œæ˜¯å¦è§¦å‘å¼‚å¸¸æ¨¡å¼ï¼š
    - çŸ­æ—¶é—´å†…å¤šæ¬¡æŠ˜æ‰£ç”³è¯·
    - è¥æ”¶çªç„¶å¼‚å¸¸ä¸‹é™
    """
    async def _run():
        from ..services.store_memory_service import StoreMemoryService
        from ..models.store_memory import AnomalyPattern
        from datetime import datetime as _dt

        service = StoreMemoryService()
        memory = await service.get_memory(store_id)

        if not memory:
            return {"store_id": store_id, "anomaly_detected": False, "reason": "no_memory"}

        action_type = event.get("action_type", "")
        anomaly_detected = False
        anomaly_type = None

        # ç®€å•è§„åˆ™ï¼šè¿ç»­3æ¬¡ä»¥ä¸ŠæŠ˜æ‰£ç”³è¯·æ ‡è®°ä¸ºå¼‚å¸¸
        if action_type == "discount_apply":
            recent_discounts = [
                p for p in memory.anomaly_patterns
                if p.pattern_type == "frequent_discount"
            ]
            if len(recent_discounts) > 0:
                recent_discounts[0].occurrence_count += 1
                recent_discounts[0].last_seen = _dt.utcnow()
                if recent_discounts[0].occurrence_count >= 3:
                    anomaly_detected = True
                    anomaly_type = "frequent_discount"
            else:
                memory.anomaly_patterns.append(AnomalyPattern(
                    pattern_type="frequent_discount",
                    description="çŸ­æ—¶é—´å†…å¤šæ¬¡æŠ˜æ‰£ç”³è¯·",
                    first_seen=_dt.utcnow(),
                    last_seen=_dt.utcnow(),
                    severity="medium",
                ))

            await service._store.save(memory)

        logger.info(
            "realtime_anomaly_check.done",
            store_id=store_id,
            anomaly_detected=anomaly_detected,
            anomaly_type=anomaly_type,
        )

        return {
            "store_id": store_id,
            "anomaly_detected": anomaly_detected,
            "anomaly_type": anomaly_type,
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("realtime_anomaly_check_failed", store_id=store_id, error=str(e))
        raise self.retry(exc=e)


# ============================================================
# FEAT-002: é¢„æµ‹æ€§å¤‡æ–™ Celery ä»»åŠ¡
# ============================================================

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="src.core.celery_tasks.push_daily_forecast",
    max_retries=2,
    default_retry_delay=300,
)
def push_daily_forecast(self, store_id: str = None) -> Dict[str, Any]:
    """
    æ¯æ—¥9AM æ¨é€é¢„æµ‹æ€§å¤‡æ–™å»ºè®®

    Args:
        store_id: æŒ‡å®šé—¨åº—ï¼ˆNone è¡¨ç¤ºæ‰€æœ‰é—¨åº—ï¼‰

    Returns:
        æ¨é€ç»“æœ
    """
    async def _run():
        from datetime import date, timedelta
        from ..models.store import Store
        from ..core.database import get_db_session
        from ..services.demand_forecaster import DemandForecaster
        from sqlalchemy import select

        target_date = date.today() + timedelta(days=1)
        forecaster = DemandForecaster()
        pushed = 0
        low_confidence_count = 0

        async with get_db_session() as session:
            if store_id:
                result = await session.execute(
                    select(Store).where(Store.id == store_id, Store.is_active == True)
                )
                stores = result.scalars().all()
            else:
                result = await session.execute(
                    select(Store).where(Store.is_active == True)
                )
                stores = result.scalars().all()

        for store in stores:
            try:
                forecast = await forecaster.predict(
                    store_id=str(store.id),
                    target_date=target_date,
                )

                # confidence=low æ—¶æ¨é€å«"æ•°æ®ç§¯ç´¯ä¸­"æç¤º
                if forecast.confidence == "low":
                    low_confidence_count += 1
                    message = (
                        f"ã€å¤‡æ–™å»ºè®®ï¼ˆå‚è€ƒï¼‰ã€‘æ˜æ—¥ {target_date}\n"
                        f"é—¨åº—ï¼š{store.name}\n"
                        f"âš ï¸ æ•°æ®ç§¯ç´¯ä¸­ï¼ˆå†å²æ•°æ®ä¸è¶³ï¼‰ï¼Œå»ºè®®ä»¥è¿‘æœŸç»éªŒä¸ºä¸»ã€‚\n"
                        f"é¢„ä¼°è¥æ”¶ï¼šÂ¥{forecast.estimated_revenue:.0f}"
                    )
                else:
                    message = (
                        f"ã€å¤‡æ–™å»ºè®®ã€‘æ˜æ—¥ {target_date}\n"
                        f"é—¨åº—ï¼š{store.name}\n"
                        f"é¢„ä¼°è¥æ”¶ï¼šÂ¥{forecast.estimated_revenue:.0f}\n"
                        f"ç½®ä¿¡åº¦ï¼š{forecast.confidence}\n"
                        f"å»ºè®®å¤‡æ–™ï¼š{len(forecast.items)} ç±»é£Ÿæ"
                    )

                logger.info(
                    "daily_forecast.pushed",
                    store_id=str(store.id),
                    confidence=forecast.confidence,
                )
                pushed += 1

            except Exception as e:
                logger.error("daily_forecast.push_failed", store_id=str(store.id), error=str(e))

        return {
            "pushed": pushed,
            "low_confidence": low_confidence_count,
            "target_date": str(target_date),
        }

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("push_daily_forecast_failed", error=str(e))
        raise self.retry(exc=e)


# ============================================================
# INFRA-002: ä¼å¾®æ¶ˆæ¯é‡è¯• Celery ä»»åŠ¡
# ============================================================

@celery_app.task(
    base=CallbackTask,
    bind=True,
    name="src.core.celery_tasks.retry_failed_wechat_messages",
    max_retries=1,
    default_retry_delay=60,
)
def retry_failed_wechat_messages(self) -> Dict[str, Any]:
    """
    æ¯5åˆ†é’Ÿä»å‘Šè­¦é˜Ÿåˆ—å–å‡ºå¤±è´¥çš„ä¼å¾®æ¶ˆæ¯è¿›è¡Œé‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
    """
    async def _run():
        from ..services.wechat_service import wechat_service

        retried = 0
        succeeded = 0

        try:
            results = await wechat_service.retry_failed_messages(max_retries=3, batch_size=10)
            retried = results.get("retried", 0)
            succeeded = results.get("succeeded", 0)
        except Exception as e:
            logger.warning("retry_failed_wechat_messages.error", error=str(e))

        logger.info("retry_failed_wechat_messages.done", retried=retried, succeeded=succeeded)
        return {"retried": retried, "succeeded": succeeded}

    try:
        return asyncio.run(_run())
    except Exception as e:
        logger.warning("retry_failed_wechat_messages_task_failed", error=str(e))
        raise self.retry(exc=e)

